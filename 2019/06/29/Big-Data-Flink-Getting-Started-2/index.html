<!DOCTYPE html>
<!--[if lte IE 8 ]>
<html class="ie" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<!--
***************  *      *     *
      8          *    *       *
      8          *  *         *
      8          **           *
      8          *  *         *
      8          *    *       *
      8          *      *     *
      8          *        *   ***********    -----Theme By Kieran(http://go.kieran.top)
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<!--<![endif]-->

<head>
  <title>DataSet &amp; DataStream API 编程 | Thpffcj的树洞</title>
  <!-- Meta data -->
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="Hexo">
    <meta name="author" content="John Doe">
    <meta name="description" content="" />
    <meta name="keywords" content="" />

    <!-- Favicon, (keep icon in root folder) -->
    <link rel="Shortcut Icon" href="/img/favicon.ico" type="image/ico">

    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    <link rel="stylesheet" href="/css/all.css" media="screen" type="text/css">
    
    <link rel="stylesheet" href="/highlightjs/vs.css" type="text/css">
    

    <!--[if IE 8]>
    <link rel="stylesheet" type="text/css" href="/css/ie8.css" />
    <![endif]-->

    <!-- jQuery | Load our jQuery, with an alternative source fallback to a local version if request is unavailable -->
    <script src="/js/jquery-1.11.1.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/jquery-1.11.1.min.js"><\/script>')</script>

    <!-- Load these in the <head> for quicker IE8+ load times -->
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="/js/html5shiv.min.js"></script>
    <script src="/js/respond.min.js"></script>
    <![endif]-->

  
  
  

  <style>.col-md-8.col-md-offset-2.opening-statement img{display:none;}</style>
</head>

<!--
<body class="post-template">
-->
<body id="index" class="lightnav animsition">

      <!-- ============================ Off-canvas navigation =========================== -->

    <div class="sb-slidebar sb-right sb-style-overlay sb-momentum-scrolling">
        <div class="sb-close" aria-label="Close Menu" aria-hidden="true">
            <img src="/img/close.png" alt="Close">
        </div>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu">
            <li><a href="/" class="animsition-link" title="Home">Home</a></li>
            <li><a href="/archives" class="animsition-link" title="archive">archives</a></li>
            <!-- Dropdown Menu -->
			 
            <li>
                <a class="sb-toggle-submenu">Works<span class="sb-caret"></span></a>
                <ul class="sb-submenu">
                    
                        <li><a href="/" target="_BLANK" class="animsition-link">AAA</a></li>
                    
                        <li><a href="/atom.xml" target="_BLANK" class="animsition-link">BBB</a></li>
                    
                </ul>
            </li>
            
            
            
            <li>
                <a class="sb-toggle-submenu">Links<span class="sb-caret"></span></a>
                <ul class="sb-submenu">
                    
                    <li><a href="http://go.kieran.top/" class="animsition-link">Kieran</a></li>
                    
                    <li><a href="http://domain.com/" class="animsition-link">Name</a></li>
                    
                </ul>
            </li>
            
        </ul>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu secondary">
            <li><a href="/about.html" class="animsition-link" title="about">About</a></li>
            <li><a href="/atom.xml" class="animsition-link" title="rss">RSS</a></li>
        </ul>
    </div>
    
    <!-- ============================ END Off-canvas navigation =========================== -->

    <!-- ============================ #sb-site Main Page Wrapper =========================== -->

    <div id="sb-site">
        <!-- #sb-site - All page content should be contained within this id, except the off-canvas navigation itself -->

        <!-- ============================ Header & Logo bar =========================== -->

        <div id="navigation" class="navbar navbar-fixed-top">
            <div class="navbar-inner">
                <div class="container">
                    <!-- Nav logo -->
                    <div class="logo">
                        <a href="/" title="Logo" class="animsition-link">
                         <img src="/img/logo.png" alt="Logo" width="35px;"> 
                        </a>
                    </div>
                    <!-- // Nav logo -->
                    <!-- Info-bar -->
                    <nav>
                        <ul class="nav">
                            <li><a href="/" class="animsition-link">Thpffcj</a></li>
                            <li class="nolink"><span>Always </span>Creative.</li>
                            
                            <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a></li>
                            
                            
                            <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a></li>
                            
                            
                            <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a></li>
                            
                            
                            <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a></li>
                            
                            
                            <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a></li>
                            
                            <li class="nolink"><span>Welcome!</span></li>
                        </ul>
                    </nav>
                    <!--// Info-bar -->
                </div>
                <!-- // .container -->
                <div class="learnmore sb-toggle-right">More</div>
                <button type="button" class="navbar-toggle menu-icon sb-toggle-right" title="More">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar before"></span>
                <span class="icon-bar main"></span>
                <span class="icon-bar after"></span>
                </button>
            </div>
            <!-- // .navbar-inner -->
        </div>

        <!-- ============================ Header & Logo bar =========================== -->

</div>
      
<section id="intro">
    <div class="container">
        <div class="row col-md-offset-2">
            <div class="col-md-8">
    			<span class="post-meta">
      <time datetime="2019-06-29T01:20:42.000Z" itemprop="datePublished">
          2019-06-29
      </time>
    
</span>
                <h1>DataSet & DataStream API 编程</h1>
            </div>
        </div>
        <div class="col-md-8 col-md-offset-2">
      		<ul>
<li>下面是这段时间系统了解Flink的一些记录<ul>
<li><a href="http://www.thpffcj.com/2019/06/28/Big-Data-Flink-Getting-Started-1/" target="_blank" rel="noopener">初识Flink</a></li>
<li><a href="http://www.thpffcj.com/2019/06/29/Big-Data-Flink-Getting-Started-2" target="_blank" rel="noopener">DataSet &amp; DataStream API 编程</a></li>
<li><a href="http://www.thpffcj.com/2019/07/04/Big-Data-Flink-Getting-Started-3" target="_blank" rel="noopener">Flink Table API 和 Time 操作</a></li>
<li><a href="http://www.thpffcj.com/2019/07/07/Big-Data-Flink-Getting-Started-4" target="_blank" rel="noopener">Filnk Connectors 和部署</a></li>
<li><a href="http://www.thpffcj.com/2019/07/10/Big-Data-Flink-Getting-Started-5" target="_blank" rel="noopener">Flink项目实战</a></li>
</ul>
</li>
</ul>
<p>我们在初步了解了Flink和它的编程模型之后，我们学习使用DataSet API和DataStream API来使用Flink进行基本的批处理和流处理。</p>
<hr>
<h2 id="1-DataSet-API编程"><a href="#1-DataSet-API编程" class="headerlink" title="1. DataSet API编程"></a>1. DataSet API编程</h2><h3 id="1-DataSet-API开发概述"><a href="#1-DataSet-API开发概述" class="headerlink" title="1. DataSet API开发概述"></a>1. DataSet API开发概述</h3><ul>
<li>DataSet programs in Flink are regular programs that implement transformations on data sets (e.g., filtering, mapping, joining, grouping).</li>
<li>The data sets are initially created from certain sources (e.g., by reading files, or from local collections). </li>
<li>Results are returned via sinks, which may for example write the data to (distributed) files, or to standard output (for example the command line terminal).</li>
<li>Source：源/源头</li>
<li>Sink：目的地</li>
<li>Source =&gt; Flink(transformations) =&gt; Sink</li>
</ul>
<h3 id="2-DataSource"><a href="#2-DataSource" class="headerlink" title="2. DataSource"></a>2. DataSource</h3><ul>
<li>Data sources create the initial data sets, such as from files or from Java collections. </li>
<li>The general mechanism of creating data sets is abstracted behind an InputFormat.</li>
<li>基于文件</li>
<li>基于集合</li>
</ul>
<p><strong>从集合创建dataset</strong></p>
<ul>
<li>Scala实现</li>
</ul>
<pre><code>object DataSetDataSourceApp {

  def main(args: Array[String]): Unit = {

    val env = ExecutionEnvironment.getExecutionEnvironment

    fromCollection(env)
  }

  def fromCollection(env: ExecutionEnvironment): Unit = {

    import org.apache.flink.api.scala._
    val data = 1 to 10
    env.fromCollection(data).print()
  }
}
</code></pre><ul>
<li>Java实现</li>
</ul>
<pre><code>public class JavaDataSetDataSourceApp {

    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        fromCollection(env);
    }

    public static void fromCollection(ExecutionEnvironment env) throws Exception {
        List&lt;Integer&gt; list = new ArrayList&lt;&gt;();
        for (int i = 1; i &lt;= 10; i++) {
            list.add(i);
        }
        env.fromCollection(list).print();
    }
}
</code></pre><p><strong>从文件创建dataset</strong></p>
<ul>
<li>Scala实现</li>
</ul>
<pre><code>def textFile(env: ExecutionEnvironment): Unit = {
  val filePath = &quot;file:///Users/thpffcj/Public/data/hello.txt&quot;
  env.readTextFile(filePath).print()
}
</code></pre><ul>
<li>Java实现</li>
</ul>
<pre><code>public static void textFile(ExecutionEnvironment env) throws Exception {
    String filePath = &quot;file:///Users/thpffcj/Public/data/hello.txt&quot;;
    env.readTextFile(filePath).print();
}
</code></pre><p><strong>从csv文件创建dataset</strong></p>
<pre><code>case class MyCaseClass(name:String, age:Int)

def csvFile(env: ExecutionEnvironment): Unit = {

  import org.apache.flink.api.scala._
  val filePath = &quot;file:///Users/thpffcj/Public/data/people.csv&quot;

  env.readCsvFile[(String, Int, String)](filePath, ignoreFirstLine = true).print()

  env.readCsvFile[(String, Int)](filePath, ignoreFirstLine = true, includedFields = Array(0, 1)).print()

  env.readCsvFile[MyCaseClass](filePath, ignoreFirstLine = true, includedFields = Array(0, 1)).print()

  env.readCsvFile[Person](filePath, ignoreFirstLine = true, pojoFields = Array(&quot;name&quot;, &quot;age&quot;, &quot;work&quot;)).print()
}
</code></pre><p><strong>从递归文件夹创建dataset</strong></p>
<pre><code>def readRecursiveFiles(env: ExecutionEnvironment): Unit = {
  val filePath = &quot;file:///Users/thpffcj/Public/data/nested&quot;
  val parameters = new Configuration()
  parameters.setBoolean(&quot;recursive.file.enumeration&quot;, true)
  env.readTextFile(filePath).withParameters(parameters).print()
}
</code></pre><p><strong>从压缩文件中创建dataset</strong></p>
<ul>
<li>Flink currently supports transparent decompression of input files if these are marked with an appropriate file extension. </li>
<li>In particular, this means that no further configuration of the input formats is necessary and any FileInputFormat support the compression, including custom input formats. </li>
<li>Please notice that compressed files might not be read in parallel, thus impacting job scalability.</li>
</ul>
<pre><code>def readCompressionFiles(env: ExecutionEnvironment): Unit = {
  val filePath = &quot;file:///Users/thpffcj/Public/data/compression&quot;
  env.readTextFile(filePath).print()
}
</code></pre><h3 id="3-Transformation"><a href="#3-Transformation" class="headerlink" title="3. Transformation"></a>3. Transformation</h3><ul>
<li>Data transformations transform one or more DataSets into a new DataSet.</li>
</ul>
<p><strong>Map</strong></p>
<p>Takes one element and produces one element.</p>
<ul>
<li>Scala</li>
</ul>
<pre><code>def mapFunction(env: ExecutionEnvironment): Unit = {

  val data = env.fromCollection(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))

  // 对data中的每个元素做一个+1操作
  data.map((x:Int) =&gt; x + 1)
  data.map((x) =&gt; x + 1)
  data.map(x =&gt; x + 1)
  data.map(_ + 1).print()
}
</code></pre><ul>
<li>Java</li>
</ul>
<pre><code>public static void mapFunction(ExecutionEnvironment env) throws Exception {
    List&lt;Integer&gt; list = new ArrayList&lt;&gt;();
    for (int i = 1; i &lt;= 10; i++) {
        list.add(i);
    }
    DataSource&lt;Integer&gt; data = env.fromCollection(list);

    data.map(new MapFunction&lt;Integer, Integer&gt;() {
        @Override
        public Integer map(Integer input) throws Exception {
            return input + 1;
        }
    }).print();
}
</code></pre><p><strong>Filter</strong></p>
<p>Evaluates a boolean function for each element and retains those for which the function returns true.</p>
<ul>
<li>Scala</li>
</ul>
<pre><code>def filterFunction(env: ExecutionEnvironment): Unit = {
  env.fromCollection(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
    .map(_ + 1)
    .filter(_ &gt; 5).print()
}
</code></pre><ul>
<li>Java</li>
</ul>
<pre><code>data.map(new MapFunction&lt;Integer, Integer&gt;() {
    @Override
    public Integer map(Integer input) throws Exception {
        return input + 1;
    }
}).filter(new FilterFunction&lt;Integer&gt;() {
    @Override
    public boolean filter(Integer input) throws Exception {
        return input &gt; 5;
    }
}).print();
</code></pre><p><strong>MapPartition</strong></p>
<p>Transforms a parallel partition in a single function call. The function get the partition as an <code>Iterator</code> and can produce an arbitrary number of result values. The number of elements in each partition depends on the degree-of-parallelism and previous operations.</p>
<ul>
<li>Scala</li>
</ul>
<pre><code>// DataSource 100个元素，把结果存储到数据库中
def mapPartitionFunction(env: ExecutionEnvironment): Unit = {
  val students = new ListBuffer[String]
  for (i &lt;- 1 to 100) {
    students.append(&quot;student: &quot; + i)
  }

  val data = env.fromCollection(students).setParallelism(5)

  data.mapPartition(x =&gt; {

    val connection = DBUtils.getConnection()
    println(connection + &quot;...&quot;)

    // TODO... 保存数据到DB

    DBUtils.returnConnection(connection)
    x
  }).print()
}
</code></pre><ul>
<li>Java</li>
</ul>
<pre><code>public static void mapPartitionFunction(ExecutionEnvironment env) throws Exception {
    List&lt;String&gt; list = new ArrayList&lt;&gt;();
    for (int i = 1; i &lt;= 10; i++) {
        list.add(&quot;student: &quot; + i);
    }
    DataSource&lt;String&gt; data = env.fromCollection(list);

    data.mapPartition(new MapPartitionFunction&lt;String, String&gt;() {
        @Override
        public void mapPartition(Iterable&lt;String&gt; inputs, Collector&lt;String&gt; out) throws Exception {
            String connection = DBUtils.getConnection();
            System.out.println(&quot;connection = &quot; + connection);
            DBUtils.returnConnection(connection);
        }
    }).print();
}
</code></pre><p><strong>First</strong></p>
<p>Returns the first n (arbitrary) elements of a data set.</p>
<pre><code>def firstFunction(env: ExecutionEnvironment): Unit = {
  val info = ListBuffer[(Int, String)]()
  info.append((1, &quot;Hadoop&quot;))
  info.append((1, &quot;Spark&quot;))
  info.append((1, &quot;Flink&quot;))
  info.append((2, &quot;Java&quot;))
  info.append((2, &quot;Spring Boot&quot;))
  info.append((3, &quot;Linux&quot;))
  info.append((4, &quot;Vue&quot;))

  val data = env.fromCollection(info)
  data.first(3).print()
  data.groupBy(0).first(2).print()
  data.groupBy(0).sortGroup(1, Order.ASCENDING).first(2).print()
}
</code></pre><p><strong>FlatMap</strong></p>
<p>Takes one element and produces zero, one, or more elements.</p>
<pre><code>def flatMapFunction(env: ExecutionEnvironment): Unit = {
  val info = ListBuffer[String]()
  info.append(&quot;hadoop,spark&quot;)
  info.append(&quot;hadoop,flink&quot;)
  info.append(&quot;flink,flink&quot;)

  val data = env.fromCollection(info)
  data.map(_.split(&quot;,&quot;)).print()
  data.flatMap(_.split(&quot;,&quot;)).print()
  data.flatMap(_.split(&quot;,&quot;)).map((_, 1)).groupBy(0).sum(1).print()
}
</code></pre><p><strong>Distinct</strong></p>
<p>Returns the distinct elements of a data set. </p>
<pre><code>def distinctFunction(env: ExecutionEnvironment): Unit = {
  val info = ListBuffer[String]()
  info.append(&quot;hadoop,spark&quot;)
  info.append(&quot;hadoop,flink&quot;)
  info.append(&quot;flink,flink&quot;)

  val data = env.fromCollection(info)

  data.flatMap(_.split(&quot;,&quot;)).distinct().print()
}
</code></pre><p><strong>Join</strong></p>
<p>Joins two data sets by creating all pairs of elements that are equal on their keys.</p>
<pre><code>def joinFunction(env: ExecutionEnvironment): Unit = {
  val info1 = ListBuffer[(Int, String)]() // 编号 名字
  info1.append((1, &quot;Thpffcj1&quot;))
  info1.append((2, &quot;Thpffcj2&quot;))
  info1.append((3, &quot;Thpffcj3&quot;))
  info1.append((4, &quot;Thpffcj4&quot;))

  val info2 = ListBuffer[(Int, String)]() // 编号 城市
  info2.append((1, &quot;南京&quot;))
  info2.append((2, &quot;北京&quot;))
  info2.append((3, &quot;上海&quot;))
  info2.append((5, &quot;成都&quot;))

  val data1 = env.fromCollection(info1)
  val data2 = env.fromCollection(info2)

  data1.join(data2).where(0).equalTo(0).apply((first, second) =&gt; {
    (first._1, first._2, second._2)
  }).print()
}
</code></pre><p><strong>OuterJoin</strong></p>
<p>Performs a left, right, or full outer join on two data sets. </p>
<pre><code>def outJoinFunction(env: ExecutionEnvironment): Unit = {
  val info1 = ListBuffer[(Int, String)]() // 编号 名字
  info1.append((1, &quot;Thpffcj1&quot;))
  info1.append((2, &quot;Thpffcj2&quot;))
  info1.append((3, &quot;Thpffcj3&quot;))
  info1.append((4, &quot;Thpffcj4&quot;))

  val info2 = ListBuffer[(Int, String)]() // 编号 城市
  info2.append((1, &quot;南京&quot;))
  info2.append((2, &quot;北京&quot;))
  info2.append((3, &quot;上海&quot;))
  info2.append((5, &quot;成都&quot;))

  val data1 = env.fromCollection(info1)
  val data2 = env.fromCollection(info2)

  data1.leftOuterJoin(data2).where(0).equalTo(0).apply((first, second) =&gt; {
    if (second == null) {
      (first._1, first._2, &quot;-&quot;)
    } else {
      (first._1, first._2, second._2)
    }
  }).print()
}
</code></pre><p><strong>cross</strong></p>
<p>Builds the Cartesian product (cross product) of two inputs, creating all pairs of elements. </p>
<pre><code>def crossFunction(env: ExecutionEnvironment): Unit = {
  val info1 = ListBuffer[String]()
  info1.append(&quot;曼联&quot;)
  info1.append(&quot;曼城&quot;)

  val info2 = ListBuffer[String]()
  info2.append(&quot;3&quot;)
  info2.append(&quot;1&quot;)
  info2.append(&quot;0&quot;)

  val data1 = env.fromCollection(info1)
  val data2 = env.fromCollection(info2)

  data1.cross(data2).print()
}
</code></pre><h3 id="4-Sink"><a href="#4-Sink" class="headerlink" title="4. Sink"></a>4. Sink</h3><p>Data sinks consume DataSets and are used to store or return them. Data sink operations are described using an OutputFormat.</p>
<pre><code>object DataSetSinkApp {

  def main(args: Array[String]): Unit = {

    val env = ExecutionEnvironment.getExecutionEnvironment

    val data = 1.to(10)
    val text = env.fromCollection(data)

    val filePath = &quot;file:///Users/thpffcj/Public/data/sink-out&quot;

    text.writeAsText(filePath, WriteMode.OVERWRITE).setParallelism(2)

    env.execute(&quot;DataSetSinkApp&quot;)
  }
}
</code></pre><h3 id="5-计数器"><a href="#5-计数器" class="headerlink" title="5. 计数器"></a>5. 计数器</h3><pre><code>object CounterApp {

  def main(args: Array[String]): Unit = {

    val env = ExecutionEnvironment.getExecutionEnvironment

    val data = env.fromElements(&quot;hadoop&quot;, &quot;spark&quot;, &quot;flink&quot;, &quot;pyspark&quot;, &quot;storm&quot;)

    val info = data.map(new RichMapFunction[String, String] {

      // step1：定义计数器
      var counter = new LongCounter()

      override def open(parameters: Configuration): Unit = {
        // step2：注册计数器
        getRuntimeContext.addAccumulator(&quot;ele-counts-scala&quot;, counter)
      }

      override def map(value: String): String = {
        counter.add(1)
        value
      }
    }).setParallelism(5)

    val filePath = &quot;file:///Users/thpffcj/Public/data/sink-scala-count-out&quot;
    info.writeAsText(filePath, WriteMode.OVERWRITE)
    val jobResult = env.execute(&quot;CounterApp&quot;)
    // step3：获取计数器
    val num = jobResult.getAccumulatorResult[Long](&quot;ele-counts-scala&quot;)

    println(&quot;num: &quot; + num)
  }
}
</code></pre><h3 id="6-分布式缓存"><a href="#6-分布式缓存" class="headerlink" title="6. 分布式缓存"></a>6. 分布式缓存</h3><p>Flink offers a distributed cache, similar to Apache Hadoop, to make files locally accessible to parallel instances of user functions. </p>
<pre><code>object DistributedCacheApp {

  def main(args: Array[String]): Unit = {

    val env = ExecutionEnvironment.getExecutionEnvironment

    val filePath = &quot;file:///Users/thpffcj/Public/data/hello.txt&quot;

    // step1：注册一个本地/HDFS文件
    env.registerCachedFile(filePath, &quot;scala-dc&quot;)

    val data = env.fromElements(&quot;hadoop&quot;, &quot;spark&quot;, &quot;flink&quot;, &quot;pyspark&quot;, &quot;storm&quot;)

    data.map(new RichMapFunction[String, String] {

      // step2：在open方法中获取到分布式缓存的内容即可
      override def open(parameters: Configuration): Unit = {
        val dcFile = getRuntimeContext.getDistributedCache().getFile(&quot;scala-dc&quot;)
        val lines = FileUtils.readLines(dcFile)

        /**
          * 此时会出现一个异常，Java集合和Scala集合不兼容的问题
          */
        import scala.collection.JavaConverters._
        for (ele &lt;- lines.asScala) {
          println(ele)
        }
      }

      override def map(value: String): String = {
        value
      }
    }).print()
  }
}
</code></pre><p><br></p>
<hr>
<h2 id="2-DataStream-API编程"><a href="#2-DataStream-API编程" class="headerlink" title="2. DataStream API编程"></a>2. DataStream API编程</h2><ul>
<li>DataStream programs in Flink are regular programs that implement transformations on data streams (e.g., filtering, updating state, defining windows, aggregating). </li>
<li>The data streams are initially created from various sources (e.g., message queues, socket streams, files). </li>
<li>Results are returned via sinks, which may for example write the data to files, or to standard output (for example the command line terminal). </li>
</ul>
<h3 id="1-Data-Source"><a href="#1-Data-Source" class="headerlink" title="1. Data Source"></a>1. Data Source</h3><p>Sources are where your program reads its input from.</p>
<ul>
<li>Flink中使用数据源：StreamExecutionEnvironment.addSource(sourceFunction)<ul>
<li>implementing the SourceFunction for non-parallel sources</li>
<li>implementing the ParallelSourceFunction interface</li>
<li>extending the RichParallelSourceFunction for parallel sources</li>
</ul>
</li>
</ul>
<p><strong>Socket-based</strong></p>
<pre><code>object DataStreamSourceApp {

  def main(args: Array[String]): Unit = {

    val env = StreamExecutionEnvironment.getExecutionEnvironment
    socketFunction(env)

    env.execute(&quot;DataStreamSourceApp&quot;)
  }

  def socketFunction(env: StreamExecutionEnvironment): Unit = {

    val data = env.socketTextStream(&quot;localhost&quot;, 9999)
    data.print()
  }
}
</code></pre><ul>
<li>我们使用netcat发送数据</li>
</ul>
<pre><code>nc -lk 9999
</code></pre><p><strong>Custom：实现自定义数据源</strong></p>
<ul>
<li>方式一：implementing the SourceFunction for non-parallel sources</li>
</ul>
<pre><code>class CustomNonParallelSourceFunction extends SourceFunction[Long]{

  var count = 1L

  var isRunning = true

  override def run(ctx: SourceFunction.SourceContext[Long]): Unit = {
    while (isRunning) {
      ctx.collect(count)
      count += 1
      Thread.sleep(1000)
    }
  }

  override def cancel(): Unit = {
    isRunning = false
  }
}
</code></pre><ul>
<li>接着我们需要将数据源添加到环境中</li>
</ul>
<pre><code>object DataStreamSourceApp {

  def main(args: Array[String]): Unit = {

    val env = StreamExecutionEnvironment.getExecutionEnvironment

    nonParallelSourceFunction(env)

    env.execute(&quot;DataStreamSourceApp&quot;)
  }

  def nonParallelSourceFunction(env: StreamExecutionEnvironment): Unit = {
    val data = env.addSource(new CustomNonParallelSourceFunction)
    data.print()
  }
}
</code></pre><ul>
<li>方式二：implementing the ParallelSourceFunction interface</li>
</ul>
<pre><code>class CustomParallelSourceFunction extends ParallelSourceFunction[Long]
</code></pre><ul>
<li>这种方式我们可以设置并行度了</li>
</ul>
<pre><code>def parallelSourceFunction(env: StreamExecutionEnvironment): Unit = {
  val data = env.addSource(new CustomParallelSourceFunction).setParallelism(2)
  data.print()
}
</code></pre><ul>
<li>方式三：extending the RichParallelSourceFunction for parallel sources</li>
</ul>
<pre><code>class CustomRichParallelSourceFunction extends RichParallelSourceFunction[Long]
</code></pre><h3 id="2-Transformation"><a href="#2-Transformation" class="headerlink" title="2. Transformation"></a>2. Transformation</h3><p>Operators transform one or more DataStreams into a new DataStream.</p>
<p><strong>Map和Filter</strong></p>
<ul>
<li>我们直接使用上面的自定义数据源产生数据</li>
</ul>
<pre><code>def filterFunction(env: StreamExecutionEnvironment): Unit = {
  val data = env.addSource(new CustomNonParallelSourceFunction)

  data.map(x =&gt;{
    println(&quot;received: &quot; + x)
    x
  }).filter(_%2 == 0).print().setParallelism(1)
}
</code></pre><p><strong>Union</strong></p>
<p>Union of two or more data streams creating a new stream containing all the elements from all the streams. </p>
<pre><code>def unionFunction(env: StreamExecutionEnvironment): Unit = {
  val data1 = env.addSource(new CustomNonParallelSourceFunction)
  val data2 = env.addSource(new CustomNonParallelSourceFunction)
  data1.union(data2).print().setParallelism(1)
}
</code></pre><p><strong>Split和Select</strong></p>
<p>Split the stream into two or more streams according to some criterion.<br>Select one or more streams from a split stream.</p>
<pre><code>def splitSelectFunction(env: StreamExecutionEnvironment): Unit = {
  val data = env.addSource(new CustomNonParallelSourceFunction)

  val splits = data.split(new OutputSelector[Long] {
    override def select(value: Long): lang.Iterable[String] = {
      val list = new util.ArrayList[String]()
      if (value % 2 == 0) {
        list.add(&quot;even&quot;)
      } else {
        list.add(&quot;odd&quot;)
      }
      list
    }
  })

  splits.select(&quot;even&quot;).print().setParallelism(1)
}
</code></pre><h3 id="3-Sink"><a href="#3-Sink" class="headerlink" title="3. Sink"></a>3. Sink</h3><p>Data sinks consume DataStreams and forward them to files, sockets, external systems, or print them. </p>
<p><strong>自定义Sink</strong></p>
<ul>
<li>需求：socket发送数据过来，把String类型转换成对象，然后把Java对象保存到MySQL数据库中</li>
<li>数据库建表</li>
</ul>
<pre><code>create table student(
    id int(11) NOT NULL AUTO_INCREMENT,
    name varchar(25),
    age int(10),
    primary key(id)
)；
</code></pre><ul>
<li>继承RichSinkFunction<t>，T是你想要写入对象的类型</t></li>
<li>重写open/close：生命周期方法</li>
<li>重写invoke：每条记录执行一次</li>
</ul>
<pre><code>public class SinkToMySQL extends RichSinkFunction&lt;Student&gt; {

    Connection connection;
    PreparedStatement preparedStatement;

    private Connection getConnection() {
        Connection conn = null;
        try {
            String url = &quot;jdbc:mysql://localhost:3306/test&quot;;
            conn = DriverManager.getConnection(url, &quot;root&quot;, &quot;00000000&quot;);
        } catch (Exception e) {
            e.printStackTrace();
        }
        return conn;
    }

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);

        connection = getConnection();
        String sql = &quot;insert into Student(id, name, age) values (?, ?, ?)&quot;;
        preparedStatement = connection.prepareStatement(sql);
    }

    // 每条记录插入时调用一次
    public void invoke(Student value, Context context) throws Exception {

        // 为前面的占位符赋值
        preparedStatement.setInt(1, value.getId());
        preparedStatement.setString(2, value.getName());
        preparedStatement.setInt(3, value.getAge());

        preparedStatement.executeUpdate();
    }

    @Override
    public void close() throws Exception {
        if(connection != null) {
            try {
                connection.close();
            } catch(Exception e) {
                e.printStackTrace();
            }
            connection = null;
        }
    }
}
</code></pre><ul>
<li>开发测试方法</li>
</ul>
<pre><code>public class JavaCustomSinkToMySQL {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource&lt;String&gt; source = env.socketTextStream(&quot;localhost&quot;, 7777);

        SingleOutputStreamOperator&lt;Student&gt; studentStream = source.map(new MapFunction&lt;String, Student&gt;() {
            @Override
            public Student map(String value) throws Exception {
                System.out.println(value);
                String[] splits = value.split(&quot;,&quot;);
                Student stu = new Student();
                stu.setId(Integer.parseInt(splits[0]));
                stu.setName(splits[1]);
                stu.setAge(Integer.parseInt(splits[2]));
                return stu;
            }
        });

        studentStream.addSink(new SinkToMySQL());

        env.execute(&quot;JavaCustomSinkToMySQL&quot;);
    }
}
</code></pre>
            <div class="clearfix"></div>
            <hr class="nogutter">
        </div>
        <nav class="pagination" role="pagination">
    
    <a class="pull-left" href="/2019/07/04/Big-Data-Flink-Getting-Started-3/" style="float: left;">
        ← Flink Table API 和 Time 操作
    </a>
    
    
    <a class="pull-right" href="/2019/06/28/Big-Data-Flink-Getting-Started-1/">
        初识Flink →
    </a>
    
</nav>

        <div class="duoshuo">


</div>
    </div>
</section>


      
<!-- ============================ Footer =========================== -->

<footer>
    <div class="container">
            <div class="copy">
				<span id="busuanzi_container_site_pv">
					本站总访问量<span id="busuanzi_value_site_pv"></span>次
				</span>	
                <p>
                    &copy; 2014<script>new Date().getFullYear()>2010&&document.write("-"+new Date().getFullYear());</script>, Content By Thpffcj.
                </p>
                <p>私は再び1日満たすためにあなたとの重要な人々を望みます</p>
            </div>
            <div class="social">
                <ul>
                    
                    <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a>&nbsp;</li>
                    
                </ul>
            </div>
            <div class="clearfix"> </div>
        </div>
</footer>

<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- ============================ END Footer =========================== -->

      <!-- Load our scripts -->
<!-- Resizable 'on-demand' full-height hero -->
<script type="text/javascript">
    var resizeHero = function () {
        var hero = $(".cover,.heightblock"),
            window1 = $(window);
        hero.css({
            "height": window1.height()
        });
    };

    resizeHero();

    $(window).resize(function () {
        resizeHero();
    });
</script>
<script src="/js/plugins.min.js"></script><!-- Bootstrap core and concatenated plugins always load here -->
<script src="/js/jquery.flexslider-min.js"></script><!-- Flexslider plugin -->
<script src="/js/scripts.js"></script><!-- Theme scripts -->


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$('#intro').find('img').each(function(){
  var alt = this.alt;

  if (alt){
    $(this).after('<span class="caption" style="display:none">' + alt + '</span>');
  }

  $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="gallery" />');
});
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

<!-- Initiate flexslider plugin -->
<script type="text/javascript">
    $(document).ready(function($) {
      (function(){
        console.log('font');
        var getCss = function(path) {
          var head = document.getElementsByTagName('head')[0];
          link = document.createElement('link');
          link.href = path;
          link.rel = 'stylesheet';
          link.type = 'text/css';
          head.appendChild(link);
        };
        getCss('https://fonts.googleapis.com/css?family=Montserrat:400,700');
        getCss('https://fonts.googleapis.com/css?family=Open+Sans:400,600');
      })();
      $('.flexslider').flexslider({
        animation: "fade",
        prevText: "",
        nextText: "",
        directionNav: true
      });
    });
</script>

</body>
</html>
