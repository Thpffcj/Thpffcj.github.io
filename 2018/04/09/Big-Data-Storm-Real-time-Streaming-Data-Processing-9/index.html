<!DOCTYPE html>
<!--[if lte IE 8 ]>
<html class="ie" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<!--
***************  *      *     *
      8          *    *       *
      8          *  *         *
      8          **           *
      8          *  *         *
      8          *    *       *
      8          *      *     *
      8          *        *   ***********    -----Theme By Kieran(http://go.kieran.top)
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<!--<![endif]-->

<head>
  <title>Storm综合项目实战 | Thpffcj的树洞</title>
  <!-- Meta data -->
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="Hexo">
    <meta name="author" content="John Doe">
    <meta name="description" content="" />
    <meta name="keywords" content="" />

    <!-- Favicon, (keep icon in root folder) -->
    <link rel="Shortcut Icon" href="/img/favicon.ico" type="image/ico">

    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    <link rel="stylesheet" href="/css/all.css" media="screen" type="text/css">
    
    <link rel="stylesheet" href="/highlightjs/vs.css" type="text/css">
    

    <!--[if IE 8]>
    <link rel="stylesheet" type="text/css" href="/css/ie8.css" />
    <![endif]-->

    <!-- jQuery | Load our jQuery, with an alternative source fallback to a local version if request is unavailable -->
    <script src="/js/jquery-1.11.1.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/jquery-1.11.1.min.js"><\/script>')</script>

    <!-- Load these in the <head> for quicker IE8+ load times -->
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="/js/html5shiv.min.js"></script>
    <script src="/js/respond.min.js"></script>
    <![endif]-->

  
  
  

  <style>.col-md-8.col-md-offset-2.opening-statement img{display:none;}</style>
</head>

<!--
<body class="post-template">
-->
<body id="index" class="lightnav animsition">

      <!-- ============================ Off-canvas navigation =========================== -->

    <div class="sb-slidebar sb-right sb-style-overlay sb-momentum-scrolling">
        <div class="sb-close" aria-label="Close Menu" aria-hidden="true">
            <img src="/img/close.png" alt="Close"/>
        </div>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu">
            <li><a href="/" class="animsition-link" title="Home">Home</a></li>
            <li><a href="/archives" class="animsition-link" title="archive">archives</a></li>
            <!-- Dropdown Menu -->
			 
            <li>
                <a class="sb-toggle-submenu">Works<span class="sb-caret"></span></a>
                <ul class="sb-submenu">
                    
                        <li><a href="/" target="_BLANK" class="animsition-link">AAA</a></li>
                    
                        <li><a href="/atom.xml" target="_BLANK" class="animsition-link">BBB</a></li>
                    
                </ul>
            </li>
            
            
            
            <li>
                <a class="sb-toggle-submenu">Links<span class="sb-caret"></span></a>
                <ul class="sb-submenu">
                    
                    <li><a href="http://go.kieran.top/" class="animsition-link">Kieran</a></li>
                    
                    <li><a href="http://domain.com/" class="animsition-link">Name</a></li>
                    
                </ul>
            </li>
            
        </ul>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu secondary">
            <li><a href="/about.html" class="animsition-link" title="about">About</a></li>
            <li><a href="/atom.xml" class="animsition-link" title="rss">RSS</a></li>
        </ul>
    </div>
    
    <!-- ============================ END Off-canvas navigation =========================== -->

    <!-- ============================ #sb-site Main Page Wrapper =========================== -->

    <div id="sb-site">
        <!-- #sb-site - All page content should be contained within this id, except the off-canvas navigation itself -->

        <!-- ============================ Header & Logo bar =========================== -->

        <div id="navigation" class="navbar navbar-fixed-top">
            <div class="navbar-inner">
                <div class="container">
                    <!-- Nav logo -->
                    <div class="logo">
                        <a href="/" title="Logo" class="animsition-link">
                         <img src="/img/logo.png" alt="Logo" width="35px;"/> 
                        </a>
                    </div>
                    <!-- // Nav logo -->
                    <!-- Info-bar -->
                    <nav>
                        <ul class="nav">
                            <li><a href="/" class="animsition-link">Thpffcj</a></li>
                            <li class="nolink"><span>Always </span>Creative.</li>
                            
                            <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a></li>
                            
                            
                            <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a></li>
                            
                            
                            <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a></li>
                            
                            
                            <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a></li>
                            
                            
                            <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a></li>
                            
                            <li class="nolink"><span>Welcome!</span></li>
                        </ul>
                    </nav>
                    <!--// Info-bar -->
                </div>
                <!-- // .container -->
                <div class="learnmore sb-toggle-right">More</div>
                <button type="button" class="navbar-toggle menu-icon sb-toggle-right" title="More">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar before"></span>
                <span class="icon-bar main"></span>
                <span class="icon-bar after"></span>
                </button>
            </div>
            <!-- // .navbar-inner -->
        </div>

        <!-- ============================ Header & Logo bar =========================== -->


      
<section id="intro">
    <div class="container">
        <div class="row col-md-offset-2">
            <div class="col-md-8">
    			<span class="post-meta">
      <time datetime="2018-04-09T01:43:19.000Z" itemprop="datePublished">
          2018-04-09
      </time>
    
    
    | 
    <a href='/tags/大数据/'>大数据</a>
    
    
</span>
                <h1>Storm综合项目实战</h1>
            </div>
        </div>
        <div class="col-md-8 col-md-offset-2">
      		<ul>
<li>下面是这段时间系统了解Storm的一些记录<ul>
<li><a href="http://www.thpffcj.com/2018/03/05/Big-Data-Storm-Real-time-Streaming-Data-Processing-1/" target="_blank" rel="external">初识实时流处理Storm</a></li>
<li><a href="http://www.thpffcj.com/2018/03/18/Big-Data-Storm-Real-time-Streaming-Data-Processing-2/" target="_blank" rel="external">Storm编程</a></li>
<li><a href="http://www.thpffcj.com/2018/03/21/Big-Data-Storm-Real-time-Streaming-Data-Processing-3/" target="_blank" rel="external">Storm周边框架使用</a></li>
<li><a href="http://www.thpffcj.com/2018/03/29/Big-Data-Storm-Real-time-Streaming-Data-Processing-4/" target="_blank" rel="external">Storm架构及部署</a></li>
<li><a href="http://www.thpffcj.com/2018/03/31/Big-Data-Storm-Real-time-Streaming-Data-Processing-5/" target="_blank" rel="external">并行度</a></li>
<li><a href="http://www.thpffcj.com/2018/04/02/Big-Data-Storm-Real-time-Streaming-Data-Processing-6/" target="_blank" rel="external">分组策略与可靠性</a></li>
<li><a href="http://www.thpffcj.com/2018/04/06/Big-Data-Storm-Real-time-Streaming-Data-Processing-7/" target="_blank" rel="external">DRPC</a></li>
<li><a href="http://www.thpffcj.com/2018/04/07/Big-Data-Storm-Real-time-Streaming-Data-Processing-8/" target="_blank" rel="external">Storm整合其他大数据框架的使用</a></li>
<li><a href="http://www.thpffcj.com/2018/04/09/Big-Data-Storm-Real-time-Streaming-Data-Processing-9/" target="_blank" rel="external">Storm综合项目实战</a></li>
</ul>
</li>
</ul>
<p>历经漫长的学习，积攒了足够的知识，我们终于迎来的Storm的实战项目。学习框架只有理论是不够的，这篇博客我们就来记录一下通过所学，使用Storm和周边框架解决一个实际生活中遇到的问题。</p>
<hr>
<h2 id="1-项目概述"><a href="#1-项目概述" class="headerlink" title="1. 项目概述"></a>1. 项目概述</h2><h3 id="1-需求"><a href="#1-需求" class="headerlink" title="1. 需求"></a>1. 需求</h3><ul>
<li>需求：实时统计景区人流量并通过热力图展示<ul>
<li>交通信息化：有效管理，及时疏导</li>
<li>智慧城市</li>
</ul>
</li>
</ul>
<h3 id="2-如何采集实时人流量数据"><a href="#2-如何采集实时人流量数据" class="headerlink" title="2. 如何采集实时人流量数据"></a>2. 如何采集实时人流量数据</h3><ul>
<li>数据采集的方式<ul>
<li>GPS：获取区域经纬度信息</li>
<li>手机移动网络信令：样本容量大，覆盖范围广，数据稳定可靠，对信令的相应字段进行分析，挖掘，并结合GIS技术实现自定义区域实时人流量的智能化统计分析<ul>
<li>移动通信信令在移动通信系统中，是区别于通信用的有用信号，把话音信号以外的信号统称为”信令”</li>
<li>信令不同于用户的有用信号，用户信号是直接通过移动通信网由发信者传输到收信者的，而信令需要在移动通信网的移动台，基站，基站控制中心和移动交换中心之间传输，并对其进行分析，处理来形成一系列操作和控制</li>
</ul>
</li>
</ul>
</li>
<li>信令：通过移动用户发生的通信事件记录来判断该用户所处的位置。可以根据事件发生的区域，对用户的行为轨迹进行定义<ul>
<li>区域内 inside：用户处于目标范围内</li>
<li>区域外 outside：用户处于目标区域范围内</li>
<li>离开 leace：观察到驻留在某个区域的用户在该区域以外的某个地方发生了一个通信事件，则认为该用户离开了该区域</li>
<li>出现 appear：观察到用户在一个区域发生任何一个通信事件，则称之为该用户在该区域出现，用户在某个区域第一次出现则认为进入该区域</li>
</ul>
</li>
</ul>
<h3 id="3-项目整体架构"><a href="#3-项目整体架构" class="headerlink" title="3. 项目整体架构"></a>3. 项目整体架构</h3><p><img src="http://oseihavwm.bkt.clouddn.com/storm%E5%AE%9E%E6%97%B6%E6%B5%81%E5%A4%84%E7%90%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%B6%E6%9E%84.png" alt=""></p>
<h3 id="4-高德地图"><a href="#4-高德地图" class="headerlink" title="4. 高德地图"></a>4. 高德地图</h3><p><strong><a href="http://lbs.amap.com/" target="_blank" rel="external">高德开放平台 | 高德地图API</a></strong></p>
<ul>
<li>注册账号</li>
<li>创建新应用</li>
<li>添加新key edaee476f1aece49ed92adf8b15d3a52</li>
</ul>
<p><strong>高德地图API基本使用</strong></p>
<ul>
<li>参考官网例子，我们可以快速开发出一个map.html</li>
</ul>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;高德地图API使用&lt;/title&gt;

    &lt;link rel=&quot;stylesheet&quot; href=&quot;http://cache.amap.com/lbs/static/main1119.css&quot;/&gt;
    &lt;script src=&quot;http://webapi.amap.com/maps?v=1.4.5&amp;key=您申请的key值&quot;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div id=&quot;container&quot;&gt;&lt;/div&gt;

&lt;script&gt;
    var map = new AMap.Map(&quot;container&quot;, {
        resizeEnable: true,
        center: [116.418261, 39.921984],
        zoom: 11
    });
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><p><strong>高德地图API常用工具介绍</strong></p>
<p><a href="http://lbs.amap.com/api/javascript-api/reference/plugin" target="_blank" rel="external">工具类</a></p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;高德地图API使用&lt;/title&gt;

    &lt;link rel=&quot;stylesheet&quot; href=&quot;http://cache.amap.com/lbs/static/main1119.css&quot;/&gt;
    &lt;script src=&quot;http://webapi.amap.com/maps?v=1.4.5&amp;key=edaee476f1aece49ed92adf8b15d3a52&quot;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div id=&quot;container&quot;&gt;&lt;/div&gt;

&lt;script&gt;
    var map = new AMap.Map(&quot;container&quot;, {
        resizeEnable: true,
        center: [116.418261, 39.921984],
        zoom: 11
    });

    // 测量距离
    AMap.plugin([&quot;AMap.MouseTool&quot;],function(){ 
        var mousetool = new AMap.MouseTool(map); 
        mousetool.rule();
    });

    AMap.plugin([&apos;AMap.ToolBar&apos;,&apos;AMap.Scale&apos;,&apos;AMap.OverView&apos;],
    function(){
        map.addControl(new AMap.ToolBar());

        map.addControl(new AMap.Scale());

        map.addControl(new AMap.OverView({isOpen:true}));
    });
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><p><strong>高德地图热力图静态数据展示</strong></p>
<p><a href="http://lbs.amap.com/api/javascript-api/reference/layer#m_AMap.Heatmap" target="_blank" rel="external">AMap.Heatmap 插件</a></p>
<ul>
<li>热力图，基于第三方heatmap.js实现，以特殊高亮的形式显示数据密集程度。根据密集程度的不同，图上会呈现不同的颜色，以直观的形式展现数据密度。API引用了heatmap.js最新版本v2.0，v2.0基于新的渲染模型，具有更高的渲染效率和更强的性能。支持chrome、firefox、safari、ie9及以上浏览器。</li>
</ul>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;高德地图API使用&lt;/title&gt;

    &lt;link rel=&quot;stylesheet&quot; href=&quot;http://cache.amap.com/lbs/static/main1119.css&quot;/&gt;
    &lt;script src=&quot;http://webapi.amap.com/maps?v=1.4.5&amp;key=edaee476f1aece49ed92adf8b15d3a52&quot;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div id=&quot;container&quot;&gt;&lt;/div&gt;

&lt;script&gt;
    var map = new AMap.Map(&quot;container&quot;, {
        resizeEnable: true,
        center: [116.418261, 39.921984],
        zoom: 11
    });

    var heatmap;
    var points =[
        {&quot;lng&quot;:116.191031,&quot;lat&quot;:39.988585,&quot;count&quot;:1000},
        {&quot;lng&quot;:116.389275,&quot;lat&quot;:39.925818,&quot;count&quot;:11},
        {&quot;lng&quot;:116.287444,&quot;lat&quot;:39.810742,&quot;count&quot;:500},
        {&quot;lng&quot;:116.481707,&quot;lat&quot;:39.940089,&quot;count&quot;:13},
        {&quot;lng&quot;:116.410588,&quot;lat&quot;:39.880172,&quot;count&quot;:14},
        {&quot;lng&quot;:116.394816,&quot;lat&quot;:39.91181,&quot;count&quot;:15},
        {&quot;lng&quot;:116.416002,&quot;lat&quot;:39.952917,&quot;count&quot;:16}
    ];
    map.plugin([&quot;AMap.Heatmap&quot;],function() {      //加载热力图插件
        heatmap = new AMap.Heatmap(map, {
            radius: 50,
            opacity: [0, 0.8]
        });    //在地图对象叠加热力图
        heatmap.setDataSet({
            data:points,
            max:100
        }); //设置热力图数据集
        //具体参数见接口文档
    }); 


    AMap.plugin([&quot;AMap.ToolBar&quot;],function(){ 
        map.addControl(new AMap.ToolBar());
    });

&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><ul>
<li>我们接下来要做的就是访问数据库查出经纬度和数量</li>
<li>我们的数据可以从 个人中心 -&gt; 数据可视化 -&gt; 数据处拿到</li>
</ul>
<p><br></p>
<hr>
<h2 id="2-框架整合"><a href="#2-框架整合" class="headerlink" title="2. 框架整合"></a>2. 框架整合</h2><h3 id="1-Storm整合Kafka原理"><a href="#1-Storm整合Kafka原理" class="headerlink" title="1. Storm整合Kafka原理"></a>1. Storm整合Kafka原理</h3><p><strong><a href="http://storm.apache.org/releases/1.1.2/storm-kafka.html" target="_blank" rel="external">Storm Kafka Integration (0.8.x)</a></strong></p>
<ul>
<li>Spouts<ul>
<li>We support both Trident and core Storm spouts. For both spout implementations, we use a BrokerHost interface that tracks Kafka broker host to partition mapping and kafkaConfig that controls some Kafka related parameters.</li>
</ul>
</li>
<li>BrokerHosts：In order to initialize your Kafka spout/emitter you need to construct an instance of the marker interface BrokerHosts. Currently, we support the following two implementations:<ul>
<li>ZkHosts</li>
<li>StaticHosts</li>
</ul>
</li>
<li>KafkaConfig：The second thing needed for constructing a kafkaSpout is an instance of KafkaConfig.</li>
<li>MultiScheme：MultiScheme is an interface that dictates how the ByteBuffer consumed from Kafka gets transformed into a storm tuple. It also controls the naming of your output field.</li>
</ul>
<p><strong>实战整合</strong></p>
<ul>
<li>首先，我们需要启动zookeeper</li>
</ul>
<pre><code>[thpffcj@thpffcj bin]$ ./zkServer.sh start
JMX enabled by default
Using config: /home/thpffcj/app/zookeeper-3.4.5-cdh5.7.0/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
</code></pre><ul>
<li>启动kafka</li>
</ul>
<pre><code>[thpffcj@thpffcj bin]$ ./kafka-server-start.sh -daemon /home/thpffcj/app/kafka_2.11-0.9.0.0/config/server.properties 
[thpffcj@thpffcj bin]$ jps
3830 Kafka
3897 Jps
3707 QuorumPeerMain
</code></pre><ul>
<li>创建一个topic专门给项目使用</li>
</ul>
<pre><code>[thpffcj@thpffcj bin]$ ./kafka-topics.sh --create --zookeeper thpffcj:2181 --replication-factor 1 --partitions 1 --topic project_topic
WARNING: Due to limitations in metric names, topics with a period (&apos;.&apos;) or underscore (&apos;_&apos;) could collide. To avoid issues it is best to use either, but not both.
Created topic &quot;project_topic&quot;.
[thpffcj@thpffcj bin]$ kafka-topics.sh --list --zookeeper thpffcj:2181hello_topic - marked for deletion
kafka-topic
kafka_streaming_topic
kafka_topic
logstash_topic
my-replicated-topic
project_topic
streamingtopic
</code></pre><ul>
<li>测试下生产者消费者是否打通</li>
</ul>
<pre><code>[thpffcj@thpffcj bin]$ ./kafka-console-producer.sh --broker-list thpffcj:9092 --topic project_topic

[thpffcj@thpffcj bin]$ ./kafka-console-consumer.sh --zookeeper thpffcj:2181 --topic project_topic
</code></pre><ul>
<li>添加依赖</li>
</ul>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;
    &lt;artifactId&gt;storm-kafka&lt;/artifactId&gt;
    &lt;version&gt;${storm.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><ul>
<li>如果环境没有问题，我们就打开IDEA进行编程</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/8.
 * kafka整合storm测试
 */
public class StormKafkaTopo {

    public static void main(String[] args) {

        TopologyBuilder builder = new TopologyBuilder();

        // Kafka使用的zk地址
        BrokerHosts hosts = new ZkHosts(&quot;192.168.92.130:2181&quot;);
        // Kafka存储数据的topic名称
        String topic = &quot;project_topic&quot;;
        // 指定zk中的一个根目录，存储的是KafkaSpout读取数据的位置信息(offset)
        String zkRoot = &quot;/&quot; + topic;
        String id = UUID.randomUUID().toString();

        SpoutConfig spoutConfig = new SpoutConfig(hosts, topic, zkRoot, id);
        KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);

        String SPOUT_ID = KafkaSpout.class.getSimpleName();
        builder.setSpout(SPOUT_ID, kafkaSpout);

        String BOLD_ID = LogProcessBolt.class.getSimpleName();
        builder.setBolt(BOLD_ID, new LogProcessBolt()).shuffleGrouping(SPOUT_ID);

        LocalCluster cluster = new LocalCluster();
        cluster.submitTopology(StormKafkaTopo.class.getSimpleName(),
                new Config(),
                builder.createTopology());
    }
}
</code></pre><ul>
<li>开发模式和前面基本一致，需要使用的新的东西也都是模仿官网例子，这里面用了一个自己写的处理数据的LogProcessBolt</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/8.
 * 接收kafka的数据进行处理的BOLT
 */
public class LogProcessBolt extends BaseRichBolt {

    private OutputCollector collector;

    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
    }

    public void execute(Tuple input) {

        try {
            byte[] binaryByField = input.getBinaryByField(&quot;bytes&quot;);
            String value = new String(binaryByField);

            System.out.println(value + &quot;......&quot;);

            this.collector.ack(input);
        } catch (Exception e) {
            this.collector.fail(input);
        }
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }
}
</code></pre><ul>
<li>我们要注意field为什么是butes，我们可以进入KafkaSpout源码查看，可以继续追踪到RawMultiScheme</li>
</ul>
<pre><code>public class RawMultiScheme implements MultiScheme {
  @Override
  public Iterable&lt;List&lt;Object&gt;&gt; deserialize(ByteBuffer ser) {
    return asList(tuple(Utils.toByteArray(ser)));
  }

  @Override
  public Fields getOutputFields() {
    return new Fields(&quot;bytes&quot;);
  }
}
</code></pre><ul>
<li>我们初步开发就已经完成，跑起来看看是否有问题</li>
<li>发现报错</li>
</ul>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: kafka/api/OffsetRequest
</code></pre><ul>
<li>提示我们没有找到kafka的类，我们导入依赖</li>
</ul>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-2.11&lt;/artifactId&gt;
    &lt;version&gt;0.9.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><ul>
<li>发现又报错了</li>
</ul>
<pre><code>Caused by: java.lang.ClassNotFoundException: org.apache.curator.shaded.com.google.common.cache.CacheBuilder
</code></pre><ul>
<li>继续添加依赖</li>
</ul>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;
    &lt;artifactId&gt;curator-client&lt;/artifactId&gt;
    &lt;version&gt;2.12.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><ul>
<li>还是出现错误</li>
</ul>
<pre><code>SLF4J: Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError. 
java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.Log4jLoggerFactory
</code></pre><ul>
<li>错误提示jar包冲突，我们到pom文件里去解决，把冲突的相关slf4j排除</li>
</ul>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;
    &lt;artifactId&gt;storm-core&lt;/artifactId&gt;
    &lt;version&gt;${storm.version}&lt;/version&gt;
    &lt;exclusions&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt;
        &lt;/exclusion&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
        &lt;/exclusion&gt;
    &lt;/exclusions&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
    &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;
    &lt;version&gt;0.9.0.0&lt;/version&gt;
    &lt;exclusions&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
        &lt;/exclusion&gt;
    &lt;/exclusions&gt;
&lt;/dependency&gt;
</code></pre><ul>
<li>发现又缺少依赖，继续添加依赖</li>
</ul>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
    &lt;version&gt;0.9.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><ul>
<li>这时候终于成功了，我们在虚拟机生产者输入消息，可以在IDEA控制台收到，其实遇见错误不可怕，我们要善于根据报错信息解决问题</li>
</ul>
<p><strong>Storm整合kafka步骤</strong></p>
<ul>
<li>创建KafkaSpout</li>
<li>Bolt中获取到KafkaSpout发送过来的数据，field名字叫做bytes</li>
</ul>
<h3 id="2-Logstash和Kafka的整合"><a href="#2-Logstash和Kafka的整合" class="headerlink" title="2. Logstash和Kafka的整合"></a>2. Logstash和Kafka的整合</h3><ul>
<li>这个部分我们前面在学习Logstash的时候已经介绍到了，access.log是我们创建的一个空文件</li>
</ul>
<pre><code>[thpffcj@thpffcj logstash-2.4.1]$ cat project.conf 
input {
 file {
  path =&gt; &quot;/home/thpffcj/data/logs/access.log&quot;
 } 
}

output {
 kafka {
  topic_id =&gt; &quot;project_topic&quot;
  bootstrap_servers =&gt; &quot;thpffcj:9092&quot;
  batch_size =&gt; 1
 }
}
</code></pre><ul>
<li>启动Logstash</li>
</ul>
<pre><code>[thpffcj@thpffcj logstash-2.4.1]$ bin/logstash -f project.conf 
Settings: Default pipeline workers: 1
Pipeline main started
</code></pre><ul>
<li>启动kafka消费者</li>
</ul>
<pre><code>[thpffcj@thpffcj bin]$ ./kafka-console-consumer.sh --zookeeper thpffcj:2181 --topic project_topic
</code></pre><ul>
<li>在日志中随便输入点什么，看消费者能否接收到消息</li>
</ul>
<pre><code>[thpffcj@thpffcj logs]$ echo &quot;aa&quot; &gt;&gt; access.log

{&quot;message&quot;:&quot;aa&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2018-04-09T10:08:40.146Z&quot;,&quot;path&quot;:&quot;/home/thpffcj/data/logs/access.log&quot;,&quot;host&quot;:&quot;thpffcj&quot;}
</code></pre><ul>
<li>我们启动IDEA项目发现也能接收到消息，但是我们发现每次启动IDEA都能收到以前所有的消息</li>
<li>How KafkaSpout stores offsets of a Kafka topic and recovers in case of failures<ul>
<li>As shown in the above KafkaConfig properties, you can control from where in the Kafka topic the spout begins to read by setting KafkaConfig.startOffsetTime as follows:<ul>
<li>kafka.api.OffsetRequest.EarliestTime(): read from the beginning of the topic (i.e. from the oldest messages onwards)</li>
<li>kafka.api.OffsetRequest.LatestTime(): read from the end of the topic (i.e. any new messsages that are being written to the topic)</li>
<li>A Unix timestamp aka seconds since the epoch (e.g. via System.currentTimeMillis())</li>
</ul>
</li>
</ul>
</li>
<li>所以我们只需添加一行代码</li>
</ul>
<pre><code>SpoutConfig spoutConfig = new SpoutConfig(hosts, topic, zkRoot, id);

// 设置读取偏移量的操作
spoutConfig.startOffsetTime = kafka.api.OffsetRequest.LatestTime();

KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);
</code></pre><ul>
<li>再次发送消息，发现我们只接收到了最新的消息</li>
</ul>
<pre><code>[thpffcj@thpffcj logs]$ echo &quot;thpffcj&quot; &gt;&gt; access.log

{&quot;message&quot;:&quot;thpffcj&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2018-04-09T10:16:30.138Z&quot;,&quot;path&quot;:&quot;/home/thpffcj/data/logs/access.log&quot;,&quot;host&quot;:&quot;thpffcj&quot;}......
</code></pre><ul>
<li>其实我们需要的信息只有message，我们能否简化收到的信息呢，我们只需要修改logstash的配置文件</li>
</ul>
<pre><code>[thpffcj@thpffcj logstash-2.4.1]$ cat project.conf 
input {
 file {
  path =&gt; &quot;/home/thpffcj/data/logs/access.log&quot;
 } 
}

output {
 kafka {
  topic_id =&gt; &quot;project_topic&quot;
  bootstrap_servers =&gt; &quot;thpffcj:9092&quot;
  batch_size =&gt; 1
  codec =&gt; plain {
    format =&gt; &quot;%{message}&quot;
  }
 }
}
</code></pre><ul>
<li>进行测试</li>
</ul>
<pre><code>[thpffcj@thpffcj logs]$ echo &quot;Thpffcj&quot; &gt;&gt; access.log 

Thpffcj......
</code></pre><p><strong>logstash和kafka整合</strong></p>
<ul>
<li>为什么会一直从头消费kafka message</li>
<li>通过配置使得logstash发送的消息简化</li>
</ul>
<h3 id="3-数据源产生器开发"><a href="#3-数据源产生器开发" class="headerlink" title="3. 数据源产生器开发"></a>3. 数据源产生器开发</h3><ul>
<li>看过之前学习spark streaming学习过程的人应该知道，我们准备用python造一点数据</li>
</ul>
<pre><code># _*_ coding: utf-8 _*_
__author__ = &apos;Thpffcj&apos;

import random
import time

infos = [
    &quot;116.397026,39.918058&quot;,
    &quot;116.410886,39.881949&quot;,
    &quot;116.272876,39.99243&quot;,
    &quot;116.544079,40.417555&quot;,
    &quot;116.225404,40.258186&quot;,
    &quot;116.38631,39.937209&quot;,
    &quot;116.399466,39.989743&quot;
]

phones = [
    &quot;13888888888&quot;, &quot;13877777777&quot;, &quot;13866666666&quot;,
    &quot;13988888888&quot;, &quot;13977777777&quot;, &quot;13966666666&quot;,
    &quot;13788888888&quot;, &quot;13777777777&quot;, &quot;13766666666&quot;,
    &quot;13688888888&quot;, &quot;13677777777&quot;, &quot;13666666666&quot;,
]

def sample_phone():
    return random.sample(phones, 1)[0]


def sample_info():
    return random.sample(infos, 1)[0]


def generate_log(count = 3):
    time_str = time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime())
    f = open(&quot;/home/thpffcj/data/logs/access.log&quot;, &quot;w+&quot;)
    while count &gt;= 1:
        query_log = &quot;{phone}\t{info}\t[{local_time}]&quot;.format(phone = sample_phone(),
                                                             info = sample_info(), local_time = time_str)
        # print(query_log)
        f.write(query_log + &quot;\t&quot;)
        count = count - 1


if __name__ == &apos;__main__&apos;:
    generate_log(10)
</code></pre><ul>
<li>产生数据格式如下：</li>
</ul>
<pre><code>13788888888    116.410886,39.881949    [2018-04-09 21:02:09]
13677777777    116.272876,39.99243    [2018-04-09 21:02:09]
13977777777    116.410886,39.881949    [2018-04-09 21:02:09]
13977777777    116.397026,39.918058    [2018-04-09 21:02:09]
13777777777    116.397026,39.918058    [2018-04-09 21:02:09]
13877777777    116.272876,39.99243    [2018-04-09 21:02:09]
13688888888    116.225404,40.258186    [2018-04-09 21:02:09]
13688888888    116.544079,40.417555    [2018-04-09 21:02:09]
13877777777    116.38631,39.937209    [2018-04-09 21:02:09]
13777777777    116.272876,39.99243    [2018-04-09 21:02:09]
</code></pre><p><br></p>
<hr>
<h2 id="3-项目开发"><a href="#3-项目开发" class="headerlink" title="3. 项目开发"></a>3. 项目开发</h2><h3 id="1-处理数据"><a href="#1-处理数据" class="headerlink" title="1. 处理数据"></a>1. 处理数据</h3><ul>
<li>启动zookeeper，启动kafka，启动logstash</li>
</ul>
<pre><code>[thpffcj@thpffcj bin]$ ./zkServer.sh start

[thpffcj@thpffcj bin]$ ./kafka-server-start.sh -daemon /home/thpffcj/app/kafka_2.11-0.9.0.0/config/server.properties 

[thpffcj@thpffcj logstash-2.4.1]$ bin/logstash -f project.conf
</code></pre><ul>
<li>启动IDEA项目，并执行Python脚本，在控制台可以看到输出</li>
</ul>
<pre><code>13677777777    116.410886,39.881949    [2018-04-10 15:15:01]......
13777777777    116.225404,40.258186    [2018-04-10 15:15:01]......
13677777777    116.410886,39.881949    [2018-04-10 15:15:01]......
13777777777    116.399466,39.989743    [2018-04-10 15:15:01]......
13988888888    116.544079,40.417555    [2018-04-10 15:15:01]......
13977777777    116.399466,39.989743    [2018-04-10 15:15:01]......
13977777777    116.225404,40.258186    [2018-04-10 15:15:01]......
13966666666    116.397026,39.918058    [2018-04-10 15:15:01]......
13966666666    116.397026,39.918058    [2018-04-10 15:15:01]......
13888888888    116.272876,39.99243    [2018-04-10 15:15:01]......
</code></pre><ul>
<li>我们现在需要做的就是解析日志数据</li>
</ul>
<pre><code>public void execute(Tuple input) {

    try {
        byte[] binaryByField = input.getBinaryByField(&quot;bytes&quot;);
        String value = new String(binaryByField);

        // 解析出来日志信息
        String[] splits = value.split(&quot;\t&quot;);
        String phone = splits[0];
        String[] temp = splits[1].split(&quot;,&quot;);
        String longitude = temp[0];
        String latitude = temp[1];
        long time = DateUtils.getInstance().getTime(splits[2]);

        System.out.println(phone + &quot; &quot; + longitude + &quot; &quot; + latitude + &quot; &quot; + time);

        this.collector.ack(input);
    } catch (Exception e) {
        this.collector.fail(input);
    }
}
</code></pre><ul>
<li>里面用到了个自己写的时间处理工具类</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/10.
 * 时间解析工具类
 */
public class DateUtils {

    private DateUtils(){}

    private static DateUtils instance;

    public static DateUtils getInstance() {
        if (instance == null) {
            instance = new DateUtils();
        }

        return instance;
    }

    FastDateFormat format = FastDateFormat.getInstance(&quot;yyyy-MM-dd HH:mm:ss&quot;);

    public long getTime(String time) throws Exception {
        return format.parse(time.substring(1, time.length() - 1)).getTime();
    }
}
</code></pre><h3 id="2-项目处理及表结构设计"><a href="#2-项目处理及表结构设计" class="headerlink" title="2. 项目处理及表结构设计"></a>2. 项目处理及表结构设计</h3><p><strong>Storm处理的结果存储在DB中</strong></p>
<ul>
<li>想把结果通过高德地图API进行展示，那么我们需要的参数有：经纬度 + 访问次数 + 时间的概念<ul>
<li>时间：<ul>
<li>每隔多久刷新：所有的数据存数据库，在数据库中进行聚合操作</li>
<li>窗口的概念：每隔多久刷新多大窗口的数据，在storm中聚合好之后存数据库</li>
<li>我们采用第一种方法实现</li>
</ul>
</li>
<li>DB：<ul>
<li>RDBMS：MySQL</li>
<li>NoSQL</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>create table stat(
    time bigint,
    latitude double,
    longitude double
);
</code></pre><h3 id="3-Storm处理结果存储到数据库中"><a href="#3-Storm处理结果存储到数据库中" class="headerlink" title="3. Storm处理结果存储到数据库中"></a>3. Storm处理结果存储到数据库中</h3><ul>
<li>我们前面已经学习过storm整合jdbc，现在只要在前面基础上稍作修改</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/8.
 * kafka整合storm测试
 */
public class StormKafkaTopo {

    public static void main(String[] args) {

        TopologyBuilder builder = new TopologyBuilder();

        // Kafka使用的zk地址
        BrokerHosts hosts = new ZkHosts(&quot;192.168.92.130:2181&quot;);
        // Kafka存储数据的topic名称
        String topic = &quot;project_topic&quot;;
        // 指定zk中的一个根目录，存储的是KafkaSpout读取数据的位置信息(offset)
        String zkRoot = &quot;/&quot; + topic;
        String id = UUID.randomUUID().toString();

        SpoutConfig spoutConfig = new SpoutConfig(hosts, topic, zkRoot, id);

        // 设置读取偏移量的操作
        spoutConfig.startOffsetTime = kafka.api.OffsetRequest.LatestTime();

        KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);

        String SPOUT_ID = KafkaSpout.class.getSimpleName();
        builder.setSpout(SPOUT_ID, kafkaSpout);

        String BOLD_ID = LogProcessBolt.class.getSimpleName();
        builder.setBolt(BOLD_ID, new LogProcessBolt()).shuffleGrouping(SPOUT_ID);

        Map hikariConfigMap = Maps.newHashMap();
        hikariConfigMap.put(&quot;dataSourceClassName&quot;,&quot;com.mysql.jdbc.jdbc2.optional.MysqlDataSource&quot;);
        hikariConfigMap.put(&quot;dataSource.url&quot;, &quot;jdbc:mysql://localhost/storm&quot;);
        hikariConfigMap.put(&quot;dataSource.user&quot;,&quot;root&quot;);
        hikariConfigMap.put(&quot;dataSource.password&quot;,&quot;000000&quot;);
        ConnectionProvider connectionProvider = new HikariCPConnectionProvider(hikariConfigMap);

        String tableName = &quot;stat&quot;;
        JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(tableName, connectionProvider);

        JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(connectionProvider, simpleJdbcMapper)
                .withTableName(tableName)
                .withQueryTimeoutSecs(30);

        builder.setBolt(&quot;JdbcInsertBolt&quot;, userPersistanceBolt).shuffleGrouping(BOLD_ID);


        LocalCluster cluster = new LocalCluster();
        cluster.submitTopology(StormKafkaTopo.class.getSimpleName(),
                new Config(),
                builder.createTopology());
    }
}
</code></pre><ul>
<li>我们还需要去LogProcessBolt定义发送的数据</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/8.
 * 接收kafka的数据进行处理的BOLT
 */
public class LogProcessBolt extends BaseRichBolt {

    private OutputCollector collector;

    public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
    }

    public void execute(Tuple input) {

        try {
            byte[] binaryByField = input.getBinaryByField(&quot;bytes&quot;);
            String value = new String(binaryByField);

            // 解析出来日志信息
            String[] splits = value.split(&quot;\t&quot;);
            String phone = splits[0];
            String[] temp = splits[1].split(&quot;,&quot;);
            String longitude = temp[0];
            String latitude = temp[1];
            long time = DateUtils.getInstance().getTime(splits[2]);

            System.out.println(phone + &quot; &quot; + longitude + &quot; &quot; + latitude + &quot; &quot; + time);

            collector.emit(new Values(time, Double.parseDouble(longitude), Double.parseDouble(latitude)));

            this.collector.ack(input);
        } catch (Exception e) {
            this.collector.fail(input);
        }
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields(&quot;time&quot;, &quot;longitude&quot;, &quot;latitude&quot;));
    }
}
</code></pre><ul>
<li>运行python脚本，我们发现数据已经存进我们的数据库了</li>
</ul>
<h3 id="4-通过SQL完成最终结果统计"><a href="#4-通过SQL完成最终结果统计" class="headerlink" title="4. 通过SQL完成最终结果统计"></a>4. 通过SQL完成最终结果统计</h3><ul>
<li>数据已经存放在表里了，下一步就是统计处理结果</li>
<li>需求：统计最近N久的实时数量</li>
<li>我这里写了最近10小时是因为距离产生数据过去了三个小时左右，只是为了体现sql查询</li>
</ul>
<pre><code>mysql&gt; select longitude, latitude, count(1) from stat where time &gt; unix_timestamp(date_sub(current_timestamp(), interval 10 hour)) * 1000 group by longitude, latitude;
+------------+-----------+----------+
| longitude  | latitude  | count(1) |
+------------+-----------+----------+
| 116.225404 | 40.258186 |        5 |
| 116.272876 |  39.99243 |        1 |
|  116.38631 | 39.937209 |        6 |
| 116.397026 | 39.918058 |        4 |
| 116.399466 | 39.989743 |        2 |
| 116.410886 | 39.881949 |        8 |
| 116.544079 | 40.417555 |        4 |
+------------+-----------+----------+
7 rows in set
</code></pre><h3 id="5-展示数据"><a href="#5-展示数据" class="headerlink" title="5. 展示数据"></a>5. 展示数据</h3><ul>
<li>和学spark streaming时候一样，我们选用springboot展示数据</li>
<li>添加依赖</li>
</ul>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.38&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre><ul>
<li>添加配置文件</li>
</ul>
<pre><code>spring.datasource.driver-class-name: com.mysql.jdbc.Driver
spring.datasource.url: jdbc:mysql://127.0.0.1:3306/storm?useSSL=false
spring.datasource.username: root
spring.datasource.password: 000000
</code></pre><ul>
<li>根据数据库表创建实体类</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/10.
 */
public class ResultBean {

    private double lng;
    private double lat;
    private long count;

    public double getLng() {
        return lng;
    }

    public void setLng(double lng) {
        this.lng = lng;
    }

    public double getLat() {
        return lat;
    }

    public void setLat(double lat) {
        this.lat = lat;
    }

    public long getCount() {
        return count;
    }

    public void setCount(long count) {
        this.count = count;
    }
}
</code></pre><ul>
<li>编写service层查询数据</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/10.
 */
@Service
public class ResultBeanService {

    @Autowired
    JdbcTemplate jdbcTemplate;

    public List&lt;ResultBean&gt; query() {

        String sql = &quot;select longitude, latitude, count(1) as c from stat where time &gt; unix_timestamp(date_sub(current_timestamp(), interval 10 hour)) * 1000 group by longitude, latitude&quot;;

        return (List&lt;ResultBean&gt;) jdbcTemplate.query(sql, new RowMapper&lt;ResultBean&gt;() {

            @Override
            public ResultBean mapRow(ResultSet resultSet, int i) throws SQLException {
                ResultBean bean = new ResultBean();
                bean.setLng(resultSet.getDouble(&quot;longitude&quot;));
                bean.setLat(resultSet.getDouble(&quot;latitude&quot;));
                bean.setCount(resultSet.getLong(&quot;c&quot;));
                return bean;
            }
        });
    }
}
</code></pre><ul>
<li>编写controller，这里都是基本的J2EE开发Web项目了</li>
</ul>
<pre><code>/**
 * Created by Thpffcj on 2018/4/10.
 */
@RestController
public class StatApp {

    @Autowired
    ResultBeanService resultBeanService;

    @RequestMapping(value = &quot;/map&quot;, method = RequestMethod.GET)
    public ModelAndView map() {
        return new ModelAndView(&quot;map.html&quot;);
    }

    @RequestMapping(value = &quot;/map_stat&quot;, method = RequestMethod.POST)
    @ResponseBody
    public List&lt;ResultBean&gt; mapStat() {
        List&lt;ResultBean&gt; results = resultBeanService.query();
        return results;
    }
}
</code></pre><ul>
<li>map.html基本和以前一样，就是添加了个ajax异步获取数据。里面数据量*100是因为发现数据太少时无法清晰展示</li>
</ul>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;基于Storm的实时区域游客量热力图统计&lt;/title&gt;

    &lt;link rel=&quot;stylesheet&quot; href=&quot;http://cache.amap.com/lbs/static/main1119.css&quot;/&gt;
    &lt;script src=&quot;http://webapi.amap.com/maps?v=1.4.5&amp;key=edaee476f1aece49ed92adf8b15d3a52&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;js/jquery.js&quot;&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div id=&quot;container&quot;&gt;&lt;/div&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
    var map = new AMap.Map(&quot;container&quot;, {
        resizeEnable: true,
        center: [116.418261, 39.921984],
        zoom: 11
    });

    var heatmap;
    // var points =[
    //     {&quot;lng&quot;:116.191031,&quot;lat&quot;:39.988585,&quot;count&quot;:1000},
    //     {&quot;lng&quot;:116.389275,&quot;lat&quot;:39.925818,&quot;count&quot;:11},
    //     {&quot;lng&quot;:116.287444,&quot;lat&quot;:39.810742,&quot;count&quot;:500},
    //     {&quot;lng&quot;:116.481707,&quot;lat&quot;:39.940089,&quot;count&quot;:13},
    //     {&quot;lng&quot;:116.410588,&quot;lat&quot;:39.880172,&quot;count&quot;:14},
    //     {&quot;lng&quot;:116.394816,&quot;lat&quot;:39.91181,&quot;count&quot;:15},
    //     {&quot;lng&quot;:116.416002,&quot;lat&quot;:39.952917,&quot;count&quot;:16}
    // ];

    map.plugin([&quot;AMap.Heatmap&quot;], function () {      //加载热力图插件
        heatmap = new AMap.Heatmap(map, {
            radius: 50,
            opacity: [0, 0.8]
        });    //在地图对象叠加热力图
        heatmap.setDataSet({
            data: (function(){ //&lt;![CDATA[
                var points = [];
                $.ajax({
                    type: &quot;POST&quot;,
                    url: &quot;/map_stat&quot;,
                    dataType: &apos;json&apos;,
                    async: false,
                    success: function (result) {
                        for (var i = 0; i &lt; result.length; i++) {
                            points.push({&quot;lng&quot;: result[i].lng, &quot;lat&quot;: result[i].lat, &quot;count&quot;: result[i].count * 100})
                        }
                        console.log(points);
                    }
                });
                return points;
                //]]&gt;
            })(),
            max: 100
        }); //设置热力图数据集
        //具体参数见接口文档
    });

    AMap.plugin([&quot;AMap.ToolBar&quot;], function () {
        map.addControl(new AMap.ToolBar());
    });

&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><ul>
<li>项目最终效果</li>
</ul>
<p><img src="http://oseihavwm.bkt.clouddn.com/storm%E9%A1%B9%E7%9B%AE%E5%B1%95%E7%A4%BA.png" alt=""></p>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><ul>
<li>Storm<ul>
<li>Storm是什么，能干什么，业界同类型的对比</li>
<li>核心概念</li>
<li>编程</li>
<li>周边框架的使用</li>
<li>Storm的架构以及部署</li>
<li>并行度</li>
<li>分组策略</li>
<li>Storm可靠性</li>
<li>DRPC</li>
<li>Storm整合大数据框架的使用</li>
<li>Storm项目实战</li>
</ul>
</li>
</ul>

            <div class="clearfix"></div>
            <hr class="nogutter">
        </div>
        <nav class="pagination" role="pagination">
    
    <a class="pull-left" href="/2018/04/15/Big-Data-Interview/" style="float: left;">
        ← 大数据面试
    </a>
    
    
    <a class="pull-right" href="/2018/04/07/Big-Data-Storm-Real-time-Streaming-Data-Processing-8/">
        Storm整合其他大数据框架的使用 →
    </a>
    
</nav>

        <div class="duoshuo">


</div>
    </div>
</section>


      
<!-- ============================ Footer =========================== -->

<footer>
    <div class="container">
            <div class="copy">
				<span id="busuanzi_container_site_pv">
					本站总访问量<span id="busuanzi_value_site_pv"></span>次
				</span>	
                <p>
                    &copy; 2014<script>new Date().getFullYear()>2010&&document.write("-"+new Date().getFullYear());</script>, Content By Thpffcj.
                </p>
                <p>私は再び1日満たすためにあなたとの重要な人々を望みます</p>
            </div>
            <div class="social">
                <ul>
                    
                    <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a>&nbsp;</li>
                    
                </ul>
            </div>
            <div class="clearfix"> </div>
        </div>
</footer>

<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- ============================ END Footer =========================== -->

      <!-- Load our scripts -->
<!-- Resizable 'on-demand' full-height hero -->
<script type="text/javascript">
    var resizeHero = function () {
        var hero = $(".cover,.heightblock"),
            window1 = $(window);
        hero.css({
            "height": window1.height()
        });
    };

    resizeHero();

    $(window).resize(function () {
        resizeHero();
    });
</script>
<script src="/js/plugins.min.js"></script><!-- Bootstrap core and concatenated plugins always load here -->
<script src="/js/jquery.flexslider-min.js"></script><!-- Flexslider plugin -->
<script src="/js/scripts.js"></script><!-- Theme scripts -->


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$('#intro').find('img').each(function(){
  var alt = this.alt;

  if (alt){
    $(this).after('<span class="caption" style="display:none">' + alt + '</span>');
  }

  $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="gallery" />');
});
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

<!-- Initiate flexslider plugin -->
<script type="text/javascript">
    $(document).ready(function($) {
      (function(){
        console.log('font');
        var getCss = function(path) {
          var head = document.getElementsByTagName('head')[0];
          link = document.createElement('link');
          link.href = path;
          link.rel = 'stylesheet';
          link.type = 'text/css';
          head.appendChild(link);
        };
        getCss('https://fonts.googleapis.com/css?family=Montserrat:400,700');
        getCss('https://fonts.googleapis.com/css?family=Open+Sans:400,600');
      })();
      $('.flexslider').flexslider({
        animation: "fade",
        prevText: "",
        nextText: "",
        directionNav: true
      });
    });
</script>

</body>
</html>
