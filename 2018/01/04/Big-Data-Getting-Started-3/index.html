<!DOCTYPE html>
<!--[if lte IE 8 ]>
<html class="ie" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<!--
***************  *      *     *
      8          *    *       *
      8          *  *         *
      8          **           *
      8          *  *         *
      8          *    *       *
      8          *      *     *
      8          *        *   ***********    -----Theme By Kieran(http://go.kieran.top)
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<!--<![endif]-->

<head>
  <title>分布式文件系统HDFS | Thpffcj的树洞</title>
  <!-- Meta data -->
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="Hexo">
    <meta name="author" content="John Doe">
    <meta name="description" content="" />
    <meta name="keywords" content="" />

    <!-- Favicon, (keep icon in root folder) -->
    <link rel="Shortcut Icon" href="/img/favicon.ico" type="image/ico">

    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    <link rel="stylesheet" href="/css/all.css" media="screen" type="text/css">
    
    <link rel="stylesheet" href="/highlightjs/vs.css" type="text/css">
    

    <!--[if IE 8]>
    <link rel="stylesheet" type="text/css" href="/css/ie8.css" />
    <![endif]-->

    <!-- jQuery | Load our jQuery, with an alternative source fallback to a local version if request is unavailable -->
    <script src="/js/jquery-1.11.1.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/jquery-1.11.1.min.js"><\/script>')</script>

    <!-- Load these in the <head> for quicker IE8+ load times -->
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="/js/html5shiv.min.js"></script>
    <script src="/js/respond.min.js"></script>
    <![endif]-->

  
  
  

  <style>.col-md-8.col-md-offset-2.opening-statement img{display:none;}</style>
</head>

<!--
<body class="post-template">
-->
<body id="index" class="lightnav animsition">

      <!-- ============================ Off-canvas navigation =========================== -->

    <div class="sb-slidebar sb-right sb-style-overlay sb-momentum-scrolling">
        <div class="sb-close" aria-label="Close Menu" aria-hidden="true">
            <img src="/img/close.png" alt="Close">
        </div>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu">
            <li><a href="/" class="animsition-link" title="Home">Home</a></li>
            <li><a href="/archives" class="animsition-link" title="archive">archives</a></li>
            <!-- Dropdown Menu -->
			 
            <li>
                <a class="sb-toggle-submenu">Works<span class="sb-caret"></span></a>
                <ul class="sb-submenu">
                    
                        <li><a href="/" target="_BLANK" class="animsition-link">AAA</a></li>
                    
                        <li><a href="/atom.xml" target="_BLANK" class="animsition-link">BBB</a></li>
                    
                </ul>
            </li>
            
            
            
            <li>
                <a class="sb-toggle-submenu">Links<span class="sb-caret"></span></a>
                <ul class="sb-submenu">
                    
                    <li><a href="http://go.kieran.top/" class="animsition-link">Kieran</a></li>
                    
                    <li><a href="http://domain.com/" class="animsition-link">Name</a></li>
                    
                </ul>
            </li>
            
        </ul>
        <!-- Lists in Slidebars -->
        <ul class="sb-menu secondary">
            <li><a href="/about.html" class="animsition-link" title="about">About</a></li>
            <li><a href="/atom.xml" class="animsition-link" title="rss">RSS</a></li>
        </ul>
    </div>
    
    <!-- ============================ END Off-canvas navigation =========================== -->

    <!-- ============================ #sb-site Main Page Wrapper =========================== -->

    <div id="sb-site">
        <!-- #sb-site - All page content should be contained within this id, except the off-canvas navigation itself -->

        <!-- ============================ Header & Logo bar =========================== -->

        <div id="navigation" class="navbar navbar-fixed-top">
            <div class="navbar-inner">
                <div class="container">
                    <!-- Nav logo -->
                    <div class="logo">
                        <a href="/" title="Logo" class="animsition-link">
                         <img src="/img/logo.png" alt="Logo" width="35px;"> 
                        </a>
                    </div>
                    <!-- // Nav logo -->
                    <!-- Info-bar -->
                    <nav>
                        <ul class="nav">
                            <li><a href="/" class="animsition-link">Thpffcj</a></li>
                            <li class="nolink"><span>Always </span>Creative.</li>
                            
                            <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a></li>
                            
                            
                            <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a></li>
                            
                            
                            <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a></li>
                            
                            
                            <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a></li>
                            
                            
                            <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a></li>
                            
                            <li class="nolink"><span>Welcome!</span></li>
                        </ul>
                    </nav>
                    <!--// Info-bar -->
                </div>
                <!-- // .container -->
                <div class="learnmore sb-toggle-right">More</div>
                <button type="button" class="navbar-toggle menu-icon sb-toggle-right" title="More">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar before"></span>
                <span class="icon-bar main"></span>
                <span class="icon-bar after"></span>
                </button>
            </div>
            <!-- // .navbar-inner -->
        </div>

        <!-- ============================ Header & Logo bar =========================== -->

</div>
      
<section id="intro">
    <div class="container">
        <div class="row col-md-offset-2">
            <div class="col-md-8">
    			<span class="post-meta">
      <time datetime="2018-01-04T09:07:13.000Z" itemprop="datePublished">
          2018-01-04
      </time>
    
    
    | 
    <a href="/tags/大数据/">大数据</a>
    
    
</span>
                <h1>分布式文件系统HDFS</h1>
            </div>
        </div>
        <div class="col-md-8 col-md-offset-2">
      		<ul>
<li>下面是这段时间系统了解Hadoop的一些记录<ul>
<li><a href="http://www.thpffcj.com/2017/12/12/Big-Data-Getting-Started-2/" target="_blank" rel="noopener">大数据概述</a></li>
<li><a href="http://www.thpffcj.com/2018/01/04/Big-Data-Getting-Started-3/" target="_blank" rel="noopener">分布式文件系统HDFS</a></li>
<li><a href="http://www.thpffcj.com/2018/01/05/Big-Data-Getting-Started-4/" target="_blank" rel="noopener">分布式资源调度YARN</a></li>
<li><a href="http://www.thpffcj.com/2018/01/06/Big-Data-Getting-Started-5/" target="_blank" rel="noopener">分布式计算框架MapReduce</a></li>
<li><a href="http://www.thpffcj.com/2018/01/07/Big-Data-Getting-Started-6/" target="_blank" rel="noopener">Hadoop项目实战</a></li>
<li><a href="http://www.thpffcj.com/2018/01/06/Big-Data-Getting-Started-7/" target="_blank" rel="noopener">Hadoop集成Spring的使用</a></li>
<li><a href="http://www.thpffcj.com/2018/01/09/Big-Data-Getting-Started-8/" target="_blank" rel="noopener">大数据相关技术拓展</a></li>
</ul>
</li>
</ul>
<p>通过上一篇博客对Hadoop的概念介绍，我们接下来首先了解一下Hadoop核心组件之分布式文件系统HDFS。</p>
<hr>
<h2 id="1-分布式文件系统HDFS"><a href="#1-分布式文件系统HDFS" class="headerlink" title="1. 分布式文件系统HDFS"></a>1. 分布式文件系统HDFS</h2><h3 id="1-什么是HDFS"><a href="#1-什么是HDFS" class="headerlink" title="1. 什么是HDFS"></a>1. 什么是HDFS</h3><ul>
<li>Hadoop实现了一个分布式文件系统(Hadoop Distrubuted File System)，简称HDFS</li>
<li>源自于Google的GFS论文</li>
<li>发表于2003年，HDFS是GFS的克隆版</li>
</ul>
<h3 id="2-HDFS的设计目标"><a href="#2-HDFS的设计目标" class="headerlink" title="2. HDFS的设计目标"></a>2. HDFS的设计目标</h3><ul>
<li>非常巨大的分布式文件系统</li>
<li>运行在普通廉价的硬件上</li>
<li>易扩展，为用户提供性能不错的文件存储服务</li>
<li>官网：<ul>
<li>Hardware Failure</li>
<li>Streaming Data Access</li>
<li>Large Data Sets</li>
<li>Simple Coherency Model</li>
<li>Moving Computation is Cheaper than Moving Data</li>
<li>Portability Across Heterogeneous Hardware and Software Platforms</li>
</ul>
</li>
</ul>
<h3 id="3-HDFS架构"><a href="#3-HDFS架构" class="headerlink" title="3. HDFS架构"></a>3. HDFS架构</h3><p><strong>1. Master(NameNode/NN) 带 N个 slaves(DataNode/DN)</strong></p>
<p><strong>2. 1个文件会被拆分成多个Block</strong></p>
<ul>
<li>blocksize:128M（130M=&gt;2个block：128M和2M）</li>
</ul>
<p><strong>3. NN：</strong></p>
<ul>
<li>负责客户端请求的响应</li>
<li>负责元数据的(文件的名称，副本系数，Block存放的DN)管理</li>
</ul>
<p><strong>4. DN：</strong></p>
<ul>
<li>存储用户文件对应的数据块(Block)</li>
<li>要定期向NN发送心跳信息，汇报本身及其所有的block信息，健康状况</li>
</ul>
<p><strong>5. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software.The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case.</strong></p>
<p><strong>6. NameNode + N个DataNode，建议NN和DN是部署在不同的节点上</strong></p>
<p><img src="http://wx4.sinaimg.cn/mw690/e647f98bgy1g1lq8wn7g1j20q50gwmxk.jpg" alt=""></p>
<h3 id="4-HDFS副本机制"><a href="#4-HDFS副本机制" class="headerlink" title="4. HDFS副本机制"></a>4. HDFS副本机制</h3><ul>
<li>replication factor：副本系数，副本因子</li>
<li>All blocks in a file except the last block are the same size</li>
</ul>
<p><br></p>
<hr>
<h2 id="2-Hadoop环境搭建"><a href="#2-Hadoop环境搭建" class="headerlink" title="2. Hadoop环境搭建"></a>2. Hadoop环境搭建</h2><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">Hadoop: Setting up a Single Node Cluster.</a></p>
<h3 id="1-jdk安装"><a href="#1-jdk安装" class="headerlink" title="1. jdk安装"></a>1. jdk安装</h3><ul>
<li>解压</li>
<li>添加到系统环境变量</li>
<li>试环境变量生效</li>
<li>验证java是否验证成功</li>
<li>底下命令是找默认java安装位置</li>
</ul>
<pre><code>[thpffcj@localhost ~]$ which java
/usr/bin/java
[thpffcj@localhost ~]$ ls -lrt /usr/bin/java
lrwxrwxrwx. 1 root root 22 1月   2 13:01 /usr/bin/java -&gt; /etc/alternatives/java
[thpffcj@localhost ~]$ ls -lrt /etc/alternatives/java
lrwxrwxrwx. 1 root root 71 1月   2 13:01 /etc/alternatives/java -&gt; /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.102-4.b14.el7.x86_64/jre/bin/java
</code></pre><h3 id="2-安装ssh"><a href="#2-安装ssh" class="headerlink" title="2. 安装ssh"></a>2. 安装ssh</h3><pre><code>[thpffcj@localhost ~]$ yum install ssh

[thpffcj@localhost ~]$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/thpffcj/.ssh/id_rsa): 

[thpffcj@localhost ~]$ cd .ssh
[thpffcj@localhost .ssh]$ ls
id_rsa  id_rsa.pub
[thpffcj@localhost .ssh]$ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys
</code></pre><h2 id="3-下载并解压hadoop"><a href="#3-下载并解压hadoop" class="headerlink" title="3. 下载并解压hadoop"></a>3. 下载并解压hadoop</h2><ul>
<li>下载：直接去cdh网站下载</li>
</ul>
<pre><code>[thpffcj@localhost software]$ ls
hadoop-2.6.0-cdh5.7.0.tar.gz
[root@localhost software]# tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C /home/thpffcj/app/
</code></pre><ul>
<li>看一下目录结构</li>
</ul>
<pre><code>[root@localhost hadoop-2.6.0-cdh5.7.0]# ll
总用量 40
drwxr-xr-x.  2 1106 4001   137 3月  24 2016 bin
drwxr-xr-x.  2 1106 4001   166 3月  24 2016 bin-mapreduce1
drwxr-xr-x.  3 1106 4001  4096 3月  24 2016 cloudera
drwxr-xr-x.  6 1106 4001   109 3月  24 2016 etc
drwxr-xr-x.  5 1106 4001    43 3月  24 2016 examples
drwxr-xr-x.  3 1106 4001    28 3月  24 2016 examples-mapreduce1
drwxr-xr-x.  2 1106 4001   106 3月  24 2016 include
drwxr-xr-x.  3 1106 4001    20 3月  24 2016 lib
drwxr-xr-x.  2 1106 4001   239 3月  24 2016 libexec
-rw-r--r--.  1 1106 4001 17087 3月  24 2016 LICENSE.txt
-rw-r--r--.  1 1106 4001   101 3月  24 2016 NOTICE.txt
-rw-r--r--.  1 1106 4001  1366 3月  24 2016 README.txt
drwxr-xr-x.  3 1106 4001  4096 3月  24 2016 sbin
drwxr-xr-x.  4 1106 4001    31 3月  24 2016 share
drwxr-xr-x. 17 1106 4001  4096 3月  24 2016 src
</code></pre><h2 id="4-hadoop配置文件的修改-hadoop-home-etc-hadoop"><a href="#4-hadoop配置文件的修改-hadoop-home-etc-hadoop" class="headerlink" title="4. hadoop配置文件的修改(hadoop_home/etc/hadoop)"></a>4. hadoop配置文件的修改(hadoop_home/etc/hadoop)</h2><ul>
<li>修改JAVA_HOME地址</li>
</ul>
<pre><code>[root@localhost hadoop-2.6.0-cdh5.7.0]# cd etc/hadoop
[root@localhost hadoop]# ls
capacity-scheduler.xml      kms-env.sh
configuration.xsl           kms-log4j.properties
container-executor.cfg      kms-site.xml
core-site.xml               log4j.properties
hadoop-env.cmd              mapred-env.cmd
hadoop-env.sh               mapred-env.sh
hadoop-metrics2.properties  mapred-queues.xml.template
hadoop-metrics.properties   mapred-site.xml.template
hadoop-policy.xml           slaves
hdfs-site.xml               ssl-client.xml.example
httpfs-env.sh               ssl-server.xml.example
httpfs-log4j.properties     yarn-env.cmd
httpfs-signature.secret     yarn-env.sh
httpfs-site.xml             yarn-site.xml
kms-acls.xml

[root@localhost hadoop]# vi hadoop-env.sh 

# The java implementation to use.
export JAVA_HOME=/home/thpffcj/app/jdk1.8.0_151
</code></pre><ul>
<li>修改core-site.xml<ul>
<li>这里最开始我写的地址是127.0.0.1，在虚拟机上操作都是正常的，但是到后面在主机连接虚拟机里的hadoop时发现连接失败，就改成了虚拟机的ip地址(虚拟机ip地址可以使用 ip a 命令查询)</li>
</ul>
</li>
</ul>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://192.168.92.130:8020&lt;/value&gt;
    &lt;/property&gt;


    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/home/thpffcj/app/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><ul>
<li>修改hdfs-site.xml</li>
</ul>
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h3 id="5-启动"><a href="#5-启动" class="headerlink" title="5. 启动"></a>5. 启动</h3><p><strong>1. 格式化文件系统(仅第一次执行即可，不要重复执行)</strong></p>
<ul>
<li>$ bin/hdfs namenode -format</li>
</ul>
<pre><code>[root@localhost hadoop-2.6.0-cdh5.7.0]# cd bin
[root@localhost bin]# ls
hadoop  hadoop.cmd  hdfs  hdfs.cmd  mapred  mapred.cmd  rcc  yarn  yarn.cmd
[root@localhost bin]# ./hdfs namenode -format
</code></pre><p><strong>2. Start NameNode daemon and DataNode daemon:</strong></p>
<ul>
<li>$ sbin/start-dfs.sh</li>
</ul>
<pre><code>[root@localhost bin]# cd ../sbin/
[root@localhost sbin]# ./start-dfs.sh 
</code></pre><ul>
<li>查看进程</li>
</ul>
<pre><code>[root@localhost sbin]# jps
54881 NameNode
55170 SecondaryNameNode
58619 Jps
55006 DataNode
</code></pre><ul>
<li><p>我们也可以访问 <a href="http://localhost:50070" target="_blank" rel="noopener">http://localhost:50070</a> 查看是否启动成功</p>
</li>
<li><p>$ sbin/stop-dfs.sh停止服务</p>
</li>
</ul>
<pre><code>[root@localhost sbin]# ./stop-dfs.sh 
</code></pre><p><br></p>
<hr>
<h2 id="2-HDFS-shell常用命令的使用"><a href="#2-HDFS-shell常用命令的使用" class="headerlink" title="2. HDFS shell常用命令的使用"></a>2. HDFS shell常用命令的使用</h2><ul>
<li>将hadoop/bin也配置到环境变量中去，就可以在任意目录下使用hadoop相关命令，也可以不配置，在hadoop/bin目录下使用各种命令</li>
</ul>
<pre><code>[root@localhost bin]# pwd
/home/thpffcj/app/hadoop-2.6.0-cdh5.7.0/bin
[root@localhost bin]# vi ~/.bash_profile 
[root@localhost bin]# source ~/.bash_profile 
[root@localhost bin]# echo $HADOOP_HOME
/home/thpffcj/app/hadoop-2.6.0-cdh5.7.0
</code></pre><h3 id="1-查看-hdfs-dfs-命令"><a href="#1-查看-hdfs-dfs-命令" class="headerlink" title="1. 查看 hdfs dfs 命令"></a>1. 查看 hdfs dfs 命令</h3><pre><code>[root@localhost thpffcj]# hdfs dfs
Usage: hadoop fs [generic options]
    [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]
    [-cat [-ignoreCrc] &lt;src&gt; ...]
    [-checksum &lt;src&gt; ...]
    [-chgrp [-R] GROUP PATH...]
    [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]
    [-chown [-R] [OWNER][:[GROUP]] PATH...]
    [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]
    [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]
    [-count [-q] [-h] [-v] &lt;path&gt; ...]
    [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]
    [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]
    [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]
    [-df [-h] [&lt;path&gt; ...]]
    [-du [-s] [-h] &lt;path&gt; ...]
    [-expunge]
    [-find &lt;path&gt; ... &lt;expression&gt; ...]
    [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]
    [-getfacl [-R] &lt;path&gt;]
    [-getfattr [-R] {-n name | -d} [-e en] &lt;path&gt;]
    [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]
    [-help [cmd ...]]
    [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]
    [-mkdir [-p] &lt;path&gt; ...]
    [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]
    [-moveToLocal &lt;src&gt; &lt;localdst&gt;]
    [-mv &lt;src&gt; ... &lt;dst&gt;]
    [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]
    [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]
    [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]
    [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]
    [-setfacl [-R] [{-b|-k} {-m|-x &lt;acl_spec&gt;} &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]
    [-setfattr {-n name [-v value] | -x name} &lt;path&gt;]
    [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]
    [-stat [format] &lt;path&gt; ...]
    [-tail [-f] &lt;file&gt;]
    [-test -[defsz] &lt;path&gt;]
    [-text [-ignoreCrc] &lt;src&gt; ...]
    [-touchz &lt;path&gt; ...]
    [-usage [cmd ...]]

Generic options supported are
-conf &lt;configuration file&gt;     specify an application configuration file
-D &lt;property=value&gt;            use value for given property
-fs &lt;local|namenode:port&gt;      specify a namenode
-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager
-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster
-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.
-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]
</code></pre><ul>
<li>我们可以看到这些命令和linux下的命令是很相似的，当我们不知道命令时，可以什么都不写就会列出所有的命令，或者可以去官网参考各种用法</li>
</ul>
<h3 id="2-hdfs-dfs-简单命令使用"><a href="#2-hdfs-dfs-简单命令使用" class="headerlink" title="2. hdfs dfs 简单命令使用"></a>2. hdfs dfs 简单命令使用</h3><ul>
<li>进入data目录，专门存放测试数据</li>
</ul>
<pre><code>[root@localhost data]# pwd
/home/thpffcj/data
</code></pre><ul>
<li>随便写点东西</li>
</ul>
<pre><code>[root@localhost data]# vi hello.txt
[root@localhost data]# cat hello.txt 
hadoop welcome
hadoop hdfs mapreduce
hadoop hdfs
</code></pre><ul>
<li>上传文件到hdfs并查看<ul>
<li>hadoop fs 和 hdfs dfs是等价的</li>
</ul>
</li>
</ul>
<pre><code>[root@localhost data]# hdfs dfs -put hello.txt /

[root@localhost data]# hadoop fs -ls /
-rw-r--r--   1 root supergroup         49 2018-01-03 15:39 /hello.txt
</code></pre><ul>
<li>查看文件内容<ul>
<li>我们看到每次执行命令都会有warn，原因是Apache提供的hadoop本地库是32位的，而在64位的服务器上就会有问题，因此需要自己编译64位的版本。因为属于初学，感觉就先不解决warn了，出现错误之后再尝试深层次了解</li>
</ul>
</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -text /hello.txt
18/01/03 15:41:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hadoop welcome
hadoop hdfs mapreduce
hadoop hdfs
</code></pre><ul>
<li>创建文件夹</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -mkdir /test
18/01/03 18:53:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@localhost data]# hadoop fs -ls /
18/01/03 18:53:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   1 root supergroup         49 2018-01-03 15:39 /hello.txt
drwxr-xr-x   - root supergroup          0 2018-01-03 18:53 /test
</code></pre><ul>
<li>要想递归的创建目录需要加上参数-p</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -mkdir -p /test/a/b
[root@localhost data]# hadoop fs -ls -R /
18/01/03 18:56:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-rw-r--r--   1 root supergroup         49 2018-01-03 15:39 /hello.txt
drwxr-xr-x   - root supergroup          0 2018-01-03 18:55 /test
drwxr-xr-x   - root supergroup          0 2018-01-03 18:55 /test/a
drwxr-xr-x   - root supergroup          0 2018-01-03 18:55 /test/a/b
</code></pre><ul>
<li>我们使用另一个方法将本地文件拷贝到hadoop的hdfs文件系统中</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -copyFromLocal hello.txt /test/a/b/h.txt
18/01/03 18:57:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@localhost data]# hadoop fs -ls -R /
18/01/03 18:58:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-rw-r--r--   1 root supergroup         49 2018-01-03 15:39 /hello.txt
drwxr-xr-x   - root supergroup          0 2018-01-03 18:55 /test
drwxr-xr-x   - root supergroup          0 2018-01-03 18:55 /test/a
drwxr-xr-x   - root supergroup          0 2018-01-03 18:57 /test/a/b
-rw-r--r--   1 root supergroup         49 2018-01-03 18:57 /test/a/b/h.txt
</code></pre><ul>
<li>换一种方式查看文本内容</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -cat /test/a/b/h.txt
18/01/03 18:59:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hadoop welcome
hadoop hdfs mapreduce
hadoop hdfs
</code></pre><ul>
<li>将文件从hdfs拿到本地</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -get /test/a/b/h.txt
18/01/03 19:00:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@localhost data]# ls
hello.txt  h.txt
</code></pre><ul>
<li>删除文件，和linux一样，rm命令不能删除文件夹，需要加参数-R表示递归删除</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -rm /test
18/01/03 19:02:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
rm: `/test&apos;: Is a directory

[root@localhost data]# hadoop fs -rm -R /test
18/01/03 19:02:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Deleted /test
</code></pre><ul>
<li>我们现在打开浏览器到50070页面，点击utilties，发现在/目录下可以看见hello.txt</li>
<li>我们现在传一个比较大的文件</li>
</ul>
<pre><code>[root@localhost software]# ll
总用量 304284
-rw-rw-r--. 1 thpffcj thpffcj 311585484 1月   2 18:27 hadoop-2.6.0-cdh5.7.0.tar.gz

[root@localhost software]# hadoop fs -put hadoop-2.6.0-cdh5.7.0.tar.gz /
</code></pre><ul>
<li>我们看到文件分成了三个block，以为默认block大小为128M</li>
</ul>
<p><img src="http://wx1.sinaimg.cn/mw690/e647f98bgy1g1lqay6obqj20hg0beglr.jpg" alt=""></p>
<p><br></p>
<hr>
<h2 id="3-Java-API操作HDFS文件"><a href="#3-Java-API操作HDFS文件" class="headerlink" title="3. Java API操作HDFS文件"></a>3. Java API操作HDFS文件</h2><ul>
<li>首先使用IDEA创建一个maven项目</li>
<li>配置pom加载hadoop需要的jar包</li>
</ul>
<pre><code>&lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;hadoop.verson&gt;2.6.0-cdh5.7.0&lt;/hadoop.verson&gt;
&lt;/properties&gt;

&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;cloudera&lt;/id&gt;
        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.12&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
        &lt;version&gt;${hadoop.verson}&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre><ul>
<li>写一个测试用例，创建文件夹</li>
</ul>
<pre><code>package cn.edu.nju.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.net.URI;


/**
 * Created by Thpffcj on 2018/1/3.
 */
public class HDFSApp {

    public static final String HDFS_PATH = &quot;hdfs://192.168.92.130:8020&quot;;

    FileSystem fileSystem = null;
    Configuration configuration = null;

    @Before
    public void setUp() throws Exception {
        System.out.println(&quot;HDFSApp.setUp&quot;);
        configuration = new Configuration();
        fileSystem = FileSystem.get(new URI(HDFS_PATH), configuration, &quot;root&quot;);
    }

    @After
    public void tearDown() throws Exception {
        configuration = null;
        fileSystem = null;
        System.out.println(&quot;HDFSApp.tearDown&quot;);
    }

    /**
     * 创建HDFS目录
     */
    @Test
    public void mkdir() throws Exception {
        fileSystem.mkdirs(new Path(&quot;/hdfsapi/test&quot;));
    }
}
</code></pre><ul>
<li>登录虚拟机查看是否创建成功</li>
</ul>
<pre><code>[root@localhost sbin]# hadoop fs -ls -R /
18/01/03 20:49:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-rw-r--r--   1 root supergroup  311585484 2018-01-03 19:10 /hadoop-2.6.0-cdh5.7.0.tar.gz
drwxr-xr-x   - root supergroup          0 2018-01-03 20:48 /hdfsapi
drwxr-xr-x   - root supergroup          0 2018-01-03 20:48 /hdfsapi/test
-rw-r--r--   1 root supergroup         49 2018-01-03 15:39 /hello.txt
</code></pre><ul>
<li>输出一个文件到文件系统上去</li>
</ul>
<pre><code>/**
 * 创建文件
 * @throws Exception
 */
@Test
public void create() throws Exception {
    FSDataOutputStream outputStream = fileSystem.create(new Path(&quot;/hdfsapi/test/a.txt&quot;));
    outputStream.write(&quot;hello hadoop&quot;.getBytes());
    outputStream.flush();
    outputStream.close();
}
</code></pre><ul>
<li>发现报错</li>
</ul>
<pre><code>HDFSApp.setUp
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
HDFSApp.tearDown

org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /hdfsapi/test/a.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1595)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3287)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:677)
</code></pre><ul>
<li>网上给出的答案基本都在说datanode版本不一致等情况，但我发现使用jps命令，我的datanode时启动的，同时我发现在文件系统上我能看见a.txt，但大小为0，最后找到一个答案，说要关闭防火墙，关闭之后，测试成功</li>
</ul>
<pre><code>[root@localhost sbin]# systemctl stop firewalld.service
</code></pre><ul>
<li>到文件系统上读取文件进行验证</li>
</ul>
<pre><code>[root@localhost sbin]# hadoop fs -text /hdfsapi/test/a.txt
18/01/04 09:14:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hello hadoop[root@localhost sbin]#
</code></pre><ul>
<li>继续写一个读取文件的测试</li>
</ul>
<pre><code>/**
 * 查看HDFS文件的内容
 * @throws Exception
 */
@Test
public void cat() throws Exception {
    FSDataInputStream inputStream = fileSystem.open(new Path(&quot;/hdfsapi/test/a.txt&quot;));
    IOUtils.copyBytes(inputStream, System.out, 1024);
    inputStream.close();
}
</code></pre><ul>
<li>也可以成功读取</li>
</ul>
<pre><code>HDFSApp.setUp
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
hello hadoopHDFSApp.tearDown
</code></pre><ul>
<li>重命名命令</li>
</ul>
<pre><code>/**
 * 重命名
 * @throws Exception
 */
@Test
public void rename() throws Exception {
    Path oldPath = new Path(&quot;/hdfsapi/test/a.txt&quot;);
    Path newPath = new Path(&quot;/hdfsapi/test/b.txt&quot;);
    fileSystem.rename(oldPath, newPath);
}
</code></pre><ul>
<li>到文件系统上查看结果</li>
</ul>
<pre><code>[root@localhost sbin]# hadoop fs -ls /hdfsapi/test
18/01/04 09:20:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rw-r--r--   3 root supergroup         12 2018-01-04 09:10 /hdfsapi/test/b.txt
</code></pre><ul>
<li>上传本地文件到hdfs文件系统中</li>
</ul>
<pre><code>/**
 * 上传文件到HDFS
 * @throws Exception
 */
@Test
public void copyFromLocalFile() throws Exception {
    Path localPath = new Path(&quot;D:/hadoop.txt&quot;);
    Path hdfsPath = new Path(&quot;/hdfsapi/test&quot;);
    fileSystem.copyFromLocalFile(localPath, hdfsPath);
}
</code></pre><ul>
<li>登录文件系统查看情况，发现上传成功</li>
</ul>
<pre><code>[root@localhost data]# hadoop fs -cat /hdfsapi/test/hadoop.txt
18/01/04 09:47:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hello hadoop[root@localhost data]# 
</code></pre><ul>
<li>上面是上传了一个小文件，现在我们来上传一个比较大的文件，加上一个进度条</li>
</ul>
<pre><code>/**
 * 上传大文件到HDFS
 * @throws Exception
 */
@Test
public void copyFromLocalFileWithProgress() throws Exception {
    InputStream inputStream = new BufferedInputStream(
            new FileInputStream(new File(&quot;D:/java/jdk-8u151-windows-x64.exe&quot;)));

    FSDataOutputStream outputStream = fileSystem.create(new Path(&quot;/hdfsapi/test/jdk-8&quot;),
            new Progressable() {
                @Override
                // 带进度提醒信息
                public void progress() {
                    System.out.print(&quot;.&quot;);
                }
            });

    IOUtils.copyBytes(inputStream, outputStream, 4096);
}
</code></pre><ul>
<li>我们试一下从hdfs文件系统下载文件到本地</li>
</ul>
<pre><code>/**
 * 下载HDFS文件
 * @throws Exception
 */
@Test
public void copyToLocalFile() throws Exception {
    Path localPath = new Path(&quot;D:/hadoop1.txt&quot;);
    Path hdfsPath = new Path(&quot;/hdfsapi/test/hadoop.txt&quot;);
    fileSystem.copyToLocalFile(false, hdfsPath, localPath, true);
}
</code></pre><ul>
<li>查看某个目录下的所有文件</li>
</ul>
<pre><code>/**
 * 查看某个目录下的所有文件
 * @throws Exception
 */
@Test
public void listFiles() throws Exception {
    FileStatus[] fileStatuses = fileSystem.listStatus(new Path(&quot;/hdfsapi/test&quot;));

    for(FileStatus fileStatus : fileStatuses) {
        String isDir = fileStatus.isDirectory() ? &quot;文件夹&quot; : &quot;文件&quot;;
        short replication = fileStatus.getReplication();
        long len = fileStatus.getLen();
        String path = fileStatus.getPath().toString();

        System.out.println(isDir + &quot;\t&quot; + replication + &quot;\t&quot; + len + &quot;\t&quot; + path);
    }
}
</code></pre><ul>
<li>查看命令行运行结果<ul>
<li>问题：我们已经在hdfs-site.xml中设置了副本系数为1，为什么此时查询文件看到的3呢</li>
<li>如果你是通过hdfs shell的方式put上去的那么采用默认的副本系数1</li>
<li>如果使用java api上传，在本地我们并没有手工设置副本系数，所以采用的是hadoop自己的副本系数</li>
</ul>
</li>
</ul>
<pre><code>文件    3    12    hdfs://192.168.92.130:8020/hdfsapi/test/b.txt
文件    3    12    hdfs://192.168.92.130:8020/hdfsapi/test/hadoop.txt
文件    3    215956536    hdfs://192.168.92.130:8020/hdfsapi/test/jdk-8
</code></pre><ul>
<li>最后我们看一下删除操作</li>
</ul>
<pre><code>/**
 * 删除
 * @throws Exception
 */
@Test
public void delete() throws Exception{
    fileSystem.delete(new Path(&quot;/hdfsapi/test&quot;), true);
}


[root@localhost hadoop-2.6.0-cdh5.7.0]# hadoop fs -ls -R /
18/01/04 13:47:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-rw-r--r--   1 root supergroup  311585484 2018-01-03 19:10 /hadoop-2.6.0-cdh5.7.0.tar.gz
drwxr-xr-x   - root supergroup          0 2018-01-04 13:47 /hdfsapi
-rw-r--r--   1 root supergroup         49 2018-01-03 15:39 /hello.txt
</code></pre><p><br></p>
<hr>
<h2 id="3-HDFS-文件读写流程"><a href="#3-HDFS-文件读写流程" class="headerlink" title="3. HDFS 文件读写流程"></a>3. HDFS 文件读写流程</h2><h3 id="1-写数据流程"><a href="#1-写数据流程" class="headerlink" title="1. 写数据流程"></a>1. 写数据流程</h3><ul>
<li>首先我们来看几个主要的角色<ul>
<li>第一个是客户端，它是用来发起读写请求的</li>
<li>第二个角色是namenode，这个是HDFS中的一个核心，只有一个，它会全局的把控所有的请求</li>
<li>第三个角色是一堆的datanode，主要就是负责数据的存储</li>
</ul>
</li>
<li>把数据写到HDFS集群上<ul>
<li>发起一个请求</li>
<li>告诉客户端数据分块和副本个数(我们没有说明是因为客户端有默认配置，不需要客户告诉它)</li>
<li>客户端将大文件分成一系列的block</li>
<li>和namenode进行通信</li>
<li>namenode进行计算后告诉客户端我已经找到了三个datanode，分别是datanode1，datanode2，datanode3，并且是排好序的</li>
<li>客户端开始向第一个datanode写数据</li>
<li>第一个datanode会将相同的数据传送到下一个datanode去</li>
<li>每个datanode写完数据后会告诉namenode</li>
<li>namenode告诉客户端完成了第一块存储</li>
<li>所有块写完后，关闭连接</li>
</ul>
</li>
</ul>
<p><img src="http://wx4.sinaimg.cn/mw690/e647f98bgy1g1lqhlb70fj20rn0fhtcj.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw690/e647f98bgy1g1lqhuie3xj20ri0e5q6t.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw690/e647f98bgy1g1lqi53mooj20rk0e3adu.jpg" alt=""></p>
<h3 id="2-读数据流程"><a href="#2-读数据流程" class="headerlink" title="2. 读数据流程"></a>2. 读数据流程</h3><ul>
<li>从HDFS读数据<ul>
<li>发起一个请求，客户端向namenode发送文件名</li>
<li>namenode告诉客户端一系列的block和datanode的集合(第一个块存放在x1，y1，z1上)</li>
<li>客户端知道了要下载的文件包括多少块，每个块存放在哪些datanode上</li>
<li>客户端向最靠近datanode请求数据</li>
</ul>
</li>
<li>如果datanode数据丢失了或者发生异常<ul>
<li>参考下面的漫画</li>
</ul>
</li>
</ul>
<p><img src="http://wx1.sinaimg.cn/mw690/e647f98bgy1g1lqibvas4j20rl0e2adw.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw690/e647f98bgy1g1lqij2wsij20rh0f778r.jpg" alt=""></p>
<p><img src="http://wx4.sinaimg.cn/mw690/e647f98bgy1g1lqjjlaqtj20rl0e1gpy.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw690/e647f98bgy1g1lqjpb1x3j20rk0fbwiv.jpg" alt=""></p>
<p><img src="http://wx1.sinaimg.cn/mw690/e647f98bgy1g1lqju1t09j20rb0f8q7e.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw690/e647f98bgy1g1lqjzrdmsj20ri0fcdk9.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw690/e647f98bgy1g1lqk6j274j20re089myh.jpg" alt=""></p>
<h3 id="3-HDFS优缺点"><a href="#3-HDFS优缺点" class="headerlink" title="3. HDFS优缺点"></a>3. HDFS优缺点</h3><p><strong>1. 优点</strong></p>
<ul>
<li>数据冗余，硬件容错</li>
<li>处理流式的数据访问</li>
<li>适合存储大文件</li>
<li>可构建在廉价机器上</li>
</ul>
<p><strong>2. 缺点</strong></p>
<ul>
<li>低延迟的数据访问</li>
<li>小文件储存</li>
</ul>

            <div class="clearfix"></div>
            <hr class="nogutter">
        </div>
        <nav class="pagination" role="pagination">
    
    <a class="pull-left" href="/2018/01/05/Big-Data-Getting-Started-4/" style="float: left;">
        ← 分布式资源调度YARN
    </a>
    
    
    <a class="pull-right" href="/2018/01/02/Sorting-Algorithm/">
        排序算法(未完) →
    </a>
    
</nav>

        <div class="duoshuo">


</div>
    </div>
</section>


      
<!-- ============================ Footer =========================== -->

<footer>
    <div class="container">
            <div class="copy">
				<span id="busuanzi_container_site_pv">
					本站总访问量<span id="busuanzi_value_site_pv"></span>次
				</span>	
                <p>
                    &copy; 2014<script>new Date().getFullYear()>2010&&document.write("-"+new Date().getFullYear());</script>, Content By Thpffcj.
                </p>
                <p>私は再び1日満たすためにあなたとの重要な人々を望みます</p>
            </div>
            <div class="social">
                <ul>
                    
                    <li><a href="https://github.com/" title="Github" target="_blank"><i class="icon-github"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://twitter.com/" title="Twitter" target="_blank"><i class="icon-twitter"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://www.facebook.com/" title="Facebook" target="_blank"><i class="icon-facebook"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="https://google.com/" title="Google-Plus" target="_blank"><i class="icon-google-plus"></i></a>&nbsp;</li>
                    
                    
                    <li><a href="http://weibo.com/" title="Sina-Weibo" target="_blank"><i class="icon-sina-weibo"></i></a>&nbsp;</li>
                    
                </ul>
            </div>
            <div class="clearfix"> </div>
        </div>
</footer>

<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- ============================ END Footer =========================== -->

      <!-- Load our scripts -->
<!-- Resizable 'on-demand' full-height hero -->
<script type="text/javascript">
    var resizeHero = function () {
        var hero = $(".cover,.heightblock"),
            window1 = $(window);
        hero.css({
            "height": window1.height()
        });
    };

    resizeHero();

    $(window).resize(function () {
        resizeHero();
    });
</script>
<script src="/js/plugins.min.js"></script><!-- Bootstrap core and concatenated plugins always load here -->
<script src="/js/jquery.flexslider-min.js"></script><!-- Flexslider plugin -->
<script src="/js/scripts.js"></script><!-- Theme scripts -->


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$('#intro').find('img').each(function(){
  var alt = this.alt;

  if (alt){
    $(this).after('<span class="caption" style="display:none">' + alt + '</span>');
  }

  $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="gallery" />');
});
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

<!-- Initiate flexslider plugin -->
<script type="text/javascript">
    $(document).ready(function($) {
      (function(){
        console.log('font');
        var getCss = function(path) {
          var head = document.getElementsByTagName('head')[0];
          link = document.createElement('link');
          link.href = path;
          link.rel = 'stylesheet';
          link.type = 'text/css';
          head.appendChild(link);
        };
        getCss('https://fonts.googleapis.com/css?family=Montserrat:400,700');
        getCss('https://fonts.googleapis.com/css?family=Open+Sans:400,600');
      })();
      $('.flexslider').flexslider({
        animation: "fade",
        prevText: "",
        nextText: "",
        directionNav: true
      });
    });
</script>

</body>
</html>
